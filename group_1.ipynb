{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976fe6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57e1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    " \n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    " \n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567c6c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cut_content</th>\n",
       "      <th>del</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022-04-24 08:33:43</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...</td>\n",
       "      <td>疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2022-04-23 05:46:14</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...</td>\n",
       "      <td>疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 11:47:31</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...</td>\n",
       "      <td>疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 09:41:33</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...</td>\n",
       "      <td>民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21 08:54:29</td>\n",
       "      <td>1000074972</td>\n",
       "      <td>明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...</td>\n",
       "      <td>明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  #上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...  2022-04-24   \n",
       "1  #上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...  2022-04-23   \n",
       "2  #上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...  2022-04-22   \n",
       "3        #上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。   2022-04-22   \n",
       "4  明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...  2022-04-21   \n",
       "\n",
       "                  time     user_id  \\\n",
       "0  2022-04-24 08:33:43  1000074972   \n",
       "1  2022-04-23 05:46:14  1000074972   \n",
       "2  2022-04-22 11:47:31  1000074972   \n",
       "3  2022-04-22 09:41:33  1000074972   \n",
       "4  2022-04-21 08:54:29  1000074972   \n",
       "\n",
       "                                         cut_content  \\\n",
       "0  # 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...   \n",
       "1  # 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...   \n",
       "2  # 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...   \n",
       "3  # 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...   \n",
       "4  明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...   \n",
       "\n",
       "                                                 del  \n",
       "0  疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...  \n",
       "1  疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...  \n",
       "2  疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...  \n",
       "3                   民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区   \n",
       "4  明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读入数据\n",
    "group_1_del = pd.read_csv('D:\\my_research\\gensim_group\\group_1\\group_1_final.csv',encoding='utf-8')\n",
    "group_1_del.columns = ['0','content','date','time','user_id','cut_content','del']\n",
    "group_1_del = group_1_del.drop(columns='0')\n",
    "group_1_del = group_1_del.dropna(axis=0, how='any')\n",
    "group_1_del.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9e6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc4256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#完善停用词表\n",
    "#new = (['顾魏','特别','越来越','好像','我要','刚刚','天天','林之校','啊啊啊','王一博','翟潇闻','真的','哈哈哈','感觉',\n",
    "#       '不到','一点','呜呜','哈哈哈哈','好好','这是','时代','少年','余生','指教','本来','晚上','天天','不想','玖',\n",
    "#        '团肖战','代言人','马嘉祺','尹浩宇','热巴','迪丽','刘宇宁','摩登','兄弟','周杰伦','新歌','余宇涵','原创',\n",
    "#       '哔哩','.','两个','左航','周边','一善','太','岁','之恋','舞台','诶','好帅','好好看','生日快乐','美美',\n",
    "#       '卧槽','演唱会','一条','歌','救命','救','想','吃','做','说','月','买','天','号','明天','小时','分钟','夜',\n",
    "#       '穿','忘','祝','面','甜','咸','日','团','菜','愿','挺','明明','加','行','再也','不知','玩','挂','正','刷',\n",
    "#        '真','有没有','写','打卡','越','站','走','画','风','路','想到','回','王','心','话','掉','搞','干',\n",
    "#        '个头','  ','包','跳','图','份','开','字','咯','长','草','新','品牌','请','剧','哥哥','块','这部','超',\n",
    "#       '破','超话','祝','那种','%','服','周','店','圈','哒','语','高','页','一只','刘','名','度','达','也许',\n",
    "#       '综艺','先','亿','倒','水','元','拉','黑','宋亚轩','姐姐','嗨','受','录','秒','戏','俩','只会','手','焕新','聚力',\n",
    "#       '耀','荣耀','大会','视界','滚','假','气','派','中','少','视界','杨','女团','至少','额','算是','花','再也','帅',\n",
    "#        '生','之间','确实','耶','头','丢','易','烊','千玺','热','搜','道','灿烂','星汉','邓紫棋','棒','称','沉香',\n",
    "#       '如屑','集','毛','算',' ',' ','|','|','点','年','里','微光','喵','徐坤'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cac3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aaa = []\n",
    "#for line in group_1_del['del']:\n",
    "#    line_u = ' '.join(str(line).split())\n",
    "#    line_uu = line_u.split()#将一个元素按空格拆分成多个元素\n",
    "#    outstr = []\n",
    "#    for words in line_uu:\n",
    "#        if words not in new:\n",
    "            #if words in ['n','v','a','r','i']:\n",
    "#                outstr.append(words)\n",
    "#    aaa.append(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f11e2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \" \", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    text = re.sub(r\"\\[\\S+\\]\", \"\", text)      # 去除表情符号\n",
    "    text = re.sub(r\"#\\S+#\", \"\", text)      # 保留话题内容\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    text = re.sub(URL_REGEX, \"\", text)       # 去除网址\n",
    "    text = text.replace('转发微博','') # 去除无意义的词语\n",
    "    text = text.replace('网页链接','')\n",
    "    text = text.replace('的微博视频','')\n",
    "    text = text.replace('妞妞端午花草','')\n",
    "    text = text.replace('bluepoint2006','')\n",
    "    text = text.replace('上海','')\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # 合并正文中过多的空格\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b01cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in group_1_del['content']:\n",
    "    line_u = str(i)\n",
    "    line_uu = clean(line_u)\n",
    "    count.append(line_uu)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "127cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list ( filter ( None , count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0220a135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48a90120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1=res\n",
    "list2=[]\n",
    "for i in list1:\n",
    "    if i not in list2:\n",
    "        list2.append(i)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916ecdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277606"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a2ec1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbe0dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(x):\n",
    "    return (word for word, flag in pseg.cut(x) if flag in ['n'])#副词\n",
    "\n",
    "texts = [cut(a) for a in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d519786",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list ( filter ( None ,texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff116c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19dd8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2022-12-07 22:13:04,888 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "2022-12-07 22:13:04,888 : DEBUG : Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.147 seconds.\n",
      "2022-12-07 22:13:06,035 : DEBUG : Loading model cost 1.147 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2022-12-07 22:13:06,035 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(text)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8, keep_n=None)\n",
    "corpus = [dictionary.doc2bow(tmp) for tmp in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b28d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6aa299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:202: RuntimeWarning: invalid value encountered in true_divide\n",
      "  numerator = (co_occur_count / num_docs) + EPSILON\n",
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:203: RuntimeWarning: invalid value encountered in true_divide\n",
      "  denominator = (w_prime_count / num_docs) * (w_star_count / num_docs)\n",
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:198: RuntimeWarning: invalid value encountered in true_divide\n",
      "  co_doc_prob = co_occur_count / num_docs\n"
     ]
    }
   ],
   "source": [
    "# 困惑度的XY坐标\n",
    "x_perplexity = [] \n",
    "y_perplexity = [] \n",
    "# 一致性的XY坐标\n",
    "x_coherence = [] \n",
    "y_coherence = [] \n",
    "#id2word：就是上边构造的dictionary字典\n",
    "#passes：模型的训练次数\n",
    "for i in range(1, 31):     \n",
    "    # 循环生成主题数为i的模型， passes代表模型训练次数，数据量太大了，搞小一点。     \n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=i, id2word=dictionary, passes=10)     \n",
    "    # 计算当前模型困惑度\n",
    "    cur_perplexity = lda.log_perplexity(corpus_tfidf)     \n",
    "    # 构造横纵坐标轴数据\n",
    "    x_perplexity.append(i)\n",
    "    y_perplexity.append(cur_perplexity) \n",
    "    # 计算一致性\n",
    "    cv_tmp = CoherenceModel(model=lda, texts=text, dictionary=dictionary, coherence='c_v') \n",
    "    x_coherence.append(i) \n",
    "    y_coherence.append(cv_tmp.get_coherence()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制一致性折线图 \n",
    "plt.plot(x_coherence, y_coherence) \n",
    "plt.xlabel('num topics') \n",
    "plt.ylabel('coherence score') \n",
    "plt.legend(('coherence_values'), loc='best') \n",
    "plt.savefig('D:\\my_research\\gensim_group\\new_coherenceLine_1.jpg') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24afb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen = pd.DataFrame(y_coherence, x_coherence)\n",
    "cohen.to_csv('D:\\my_research\\gensim_group\\group_coherence_1.csv',encoding=\"utf-8_sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp =  pd.DataFrame(y_perplexity, x_perplexity)\n",
    "perp.to_csv('D:\\my research\\gensim_group\\group_perplexity_1.csv',encoding=\"utf-8_sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=6, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.enable_notebook()\n",
    "#data = pyLDAvis.gensim.prepare(lda, corpus_tfidf, dictionary)\n",
    "#让可视化可以在notebook内显示\n",
    "#pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b357b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
