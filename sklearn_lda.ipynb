{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63e695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e306b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f973083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地 csv 文件，可以是本地文件，也可以是远程文件\n",
    "source_csv_path = 'D:\\my_research\\gensim_group\\group_1\\group_1_final.csv'\n",
    "# 文本 csv 文件里面文本所处的列名,注意这里一定要填对，要不然会报错的！\n",
    "document_column_name = 'content'\n",
    "# 输出主题词的文件路径\n",
    "top_words_csv_path = 'D:\\my_research\\sklearn_group\\group_1words.csv'\n",
    "# 输出各文档所属主题的文件路径\n",
    "predict_topic_csv_path = 'D:\\my_research\\sklearn_group\\group_1distri.csv'\n",
    "# 可视化 html 文件路径\n",
    "html_path = 'D:\\my_research\\sklearn_group\\group_1.html'\n",
    "# 选定的主题数\n",
    "n_topics = 5\n",
    "# 要输出的每个主题的前 n_top_words 个主题词数\n",
    "n_top_words = 20\n",
    "# 去除无意义字符的正则表达式\n",
    "pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+a-zA-Z，。\\n《》、？：；“”‘’｛｝【】（）…￥！—┄－]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51eaee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_data_frame(model: LatentDirichletAllocation,\n",
    "                         tf_idf_vectorizer: TfidfVectorizer,\n",
    "                         n_top_words: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出每个主题的前 n_top_words 个词\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    tf_idf_vectorizer : sklearn 的 TfidfVectorizer\n",
    "    n_top_words :前 n_top_words 个主题词\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    rows = []\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        rows.append(top_words)\n",
    "    columns = [f'topic {i+1}' for i in range(n_top_words)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3227046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_data_frame(model: LatentDirichletAllocation, X: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出文档主题概率分布情况\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    X : 词向量矩阵\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    matrix = model.transform(X)\n",
    "    columns = [f'P(topic {i+1})' for i in range(len(model.components_))]\n",
    "    df = pd.DataFrame(matrix, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e4d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\n",
    "        source_csv_path,\n",
    "        encoding='utf-8-sig')\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\n",
    "        document_column_name: 'text'\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23b767d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cut_content</th>\n",
       "      <th>del</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022-04-24 08:33:43</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...</td>\n",
       "      <td>疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2022-04-23 05:46:14</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...</td>\n",
       "      <td>疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>#上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 11:47:31</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...</td>\n",
       "      <td>疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>#上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 09:41:33</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...</td>\n",
       "      <td>民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21 08:54:29</td>\n",
       "      <td>1000074972</td>\n",
       "      <td>明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...</td>\n",
       "      <td>明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284768</th>\n",
       "      <td>284768</td>\n",
       "      <td>#上海防疫# 梅陇镇猪肉以次充好问题引发的深思        通过“企查查”我们可以知道，上...</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>2022-04-19 11:13:14</td>\n",
       "      <td>7755234771</td>\n",
       "      <td># 上海 防疫 #   梅陇镇 猪肉 以次充好 问题 引发 的 深思            ...</td>\n",
       "      <td>防疫   梅陇镇 猪肉 以次充好 引发 深思                 企 查查 咨谕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284769</th>\n",
       "      <td>284769</td>\n",
       "      <td>#上海防疫# 大发国难财会不会被追责？！        作为封控区的一员，大家都在通过各种形...</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>2022-04-19 10:33:41</td>\n",
       "      <td>7755234771</td>\n",
       "      <td># 上海 防疫 #   大 发国难财 会 不会 被 追责 ？ ！              ...</td>\n",
       "      <td>防疫   发国难财 追责                 封控区 一员 形式 团购 跑腿 解...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284770</th>\n",
       "      <td>284770</td>\n",
       "      <td>#上海防疫# 三名老人的去世值得我们深思！“动态清零”多时的上海，在4月17日公布了三个死亡...</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>2022-04-19 02:18:01</td>\n",
       "      <td>7755234771</td>\n",
       "      <td># 上海 防疫 #   三名 老人 的 去世 值得 我们 深思 ！ “ 动态 清零 ” 多时...</td>\n",
       "      <td>防疫   三名 老人 去世 值得 深思 动态 清零 多时 月  日 公布 三个 死亡 病例 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284771</th>\n",
       "      <td>284771</td>\n",
       "      <td>#上海防疫# 核酸检测是奥密克戎的最大传播漏洞！上海为何20多天每天新冠人数持续性超2万！？...</td>\n",
       "      <td>2022-04-18</td>\n",
       "      <td>2022-04-18 11:41:17</td>\n",
       "      <td>7755234771</td>\n",
       "      <td># 上海 防疫 #   核酸 检测 是 奥密克戎 的 最大 传播 漏洞 ！ 上海 为何 20...</td>\n",
       "      <td>防疫   核酸 检测 奥密克戎 传播 漏洞  多天 新冠 人数 持续性 超 万 专家 说 病...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284772</th>\n",
       "      <td>284772</td>\n",
       "      <td>#上海防疫# 上海“鸳鸯锅”封闭至今已过了20天，部分小区甚至已封闭一个半月，但上海的疫情数...</td>\n",
       "      <td>2022-04-18</td>\n",
       "      <td>2022-04-18 02:15:27</td>\n",
       "      <td>7755234771</td>\n",
       "      <td># 上海 防疫 #   上海 “ 鸳鸯锅 ” 封闭 至今 已过 了 20 天 ， 部分 小区...</td>\n",
       "      <td>防疫   鸳鸯锅 封闭 已过  天 小区 封闭 一个半月 疫情 数字 明显好转 拐点    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284773 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "0                0  #上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...   \n",
       "1                1  #上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...   \n",
       "2                2  #上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...   \n",
       "3                3        #上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。    \n",
       "4                4  明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...   \n",
       "...            ...                                                ...   \n",
       "284768      284768  #上海防疫# 梅陇镇猪肉以次充好问题引发的深思        通过“企查查”我们可以知道，上...   \n",
       "284769      284769  #上海防疫# 大发国难财会不会被追责？！        作为封控区的一员，大家都在通过各种形...   \n",
       "284770      284770  #上海防疫# 三名老人的去世值得我们深思！“动态清零”多时的上海，在4月17日公布了三个死亡...   \n",
       "284771      284771  #上海防疫# 核酸检测是奥密克戎的最大传播漏洞！上海为何20多天每天新冠人数持续性超2万！？...   \n",
       "284772      284772  #上海防疫# 上海“鸳鸯锅”封闭至今已过了20天，部分小区甚至已封闭一个半月，但上海的疫情数...   \n",
       "\n",
       "              date                 time     user_id  \\\n",
       "0       2022-04-24  2022-04-24 08:33:43  1000074972   \n",
       "1       2022-04-23  2022-04-23 05:46:14  1000074972   \n",
       "2       2022-04-22  2022-04-22 11:47:31  1000074972   \n",
       "3       2022-04-22  2022-04-22 09:41:33  1000074972   \n",
       "4       2022-04-21  2022-04-21 08:54:29  1000074972   \n",
       "...            ...                  ...         ...   \n",
       "284768  2022-04-19  2022-04-19 11:13:14  7755234771   \n",
       "284769  2022-04-19  2022-04-19 10:33:41  7755234771   \n",
       "284770  2022-04-19  2022-04-19 02:18:01  7755234771   \n",
       "284771  2022-04-18  2022-04-18 11:41:17  7755234771   \n",
       "284772  2022-04-18  2022-04-18 02:15:27  7755234771   \n",
       "\n",
       "                                              cut_content  \\\n",
       "0       # 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...   \n",
       "1       # 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...   \n",
       "2       # 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...   \n",
       "3       # 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...   \n",
       "4       明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...   \n",
       "...                                                   ...   \n",
       "284768  # 上海 防疫 #   梅陇镇 猪肉 以次充好 问题 引发 的 深思            ...   \n",
       "284769  # 上海 防疫 #   大 发国难财 会 不会 被 追责 ？ ！              ...   \n",
       "284770  # 上海 防疫 #   三名 老人 的 去世 值得 我们 深思 ！ “ 动态 清零 ” 多时...   \n",
       "284771  # 上海 防疫 #   核酸 检测 是 奥密克戎 的 最大 传播 漏洞 ！ 上海 为何 20...   \n",
       "284772  # 上海 防疫 #   上海 “ 鸳鸯锅 ” 封闭 至今 已过 了 20 天 ， 部分 小区...   \n",
       "\n",
       "                                                      del  \n",
       "0       疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...  \n",
       "1       疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...  \n",
       "2       疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...  \n",
       "3                        民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区   \n",
       "4       明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...  \n",
       "...                                                   ...  \n",
       "284768  防疫   梅陇镇 猪肉 以次充好 引发 深思                 企 查查 咨谕...  \n",
       "284769  防疫   发国难财 追责                 封控区 一员 形式 团购 跑腿 解...  \n",
       "284770  防疫   三名 老人 去世 值得 深思 动态 清零 多时 月  日 公布 三个 死亡 病例 ...  \n",
       "284771  防疫   核酸 检测 奥密克戎 传播 漏洞  多天 新冠 人数 持续性 超 万 专家 说 病...  \n",
       "284772  防疫   鸳鸯锅 封闭 已过  天 小区 封闭 一个半月 疫情 数字 明显好转 拐点    ...  \n",
       "\n",
       "[284773 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e95d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.636 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 去重、去缺失、分词\n",
    "df['cut'] = (\n",
    "    df['text']\n",
    "    .apply(lambda x: str(x))\n",
    "    .apply(lambda x: re.sub(pattern, ' ', x))\n",
    "    .apply(lambda x: \" \".join(jieba.lcut(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4b0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造 tf-idf\n",
    "tf_idf_vectorizer = TfidfVectorizer(min_df = 0.001,max_df = 0.9)\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df['cut'])\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118bffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 tf_idf 语料训练 lda 模型\n",
    "lda.fit(tf_idf)\n",
    "\n",
    "# 计算 n_top_words 个主题词\n",
    "top_words_df = top_words_data_frame(lda, tf_idf_vectorizer, n_top_words)\n",
    "\n",
    "# 保存 n_top_words 个主题词到 csv 文件中\n",
    "top_words_df.to_csv(top_words_csv_path, encoding='utf-8-sig', index=None)\n",
    "\n",
    "# 转 tf_idf 为数组，以便后面使用它来对文本主题概率分布进行计算\n",
    "X = tf_idf.toarray()\n",
    "\n",
    "# 计算完毕主题概率分布情况\n",
    "predict_df = predict_to_data_frame(lda, X)\n",
    "\n",
    "# 保存文本主题概率分布到 csv 文件中\n",
    "predict_df.to_csv(predict_topic_csv_path, encoding='utf-8-sig', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d2f4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\pyLDAvis\\_prepare.py:228: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info  = pd.DataFrame({'saliency': saliency, 'Term': vocab, \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次生成了文件： D:\\my_research\\sklearn_group\\group_1words.csv D:\\my_research\\sklearn_group\\group_1distri.csv D:\\my_research\\sklearn_group\\group_1.html\n"
     ]
    }
   ],
   "source": [
    "# 使用 pyLDAvis 进行可视化\n",
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "pyLDAvis.save_html(data, html_path)\n",
    "# 清屏\n",
    "os.system('clear')\n",
    "# 浏览器打开 html 文件以查看可视化结果\n",
    "os.system(f'start {html_path}')\n",
    "\n",
    "print('本次生成了文件：',\n",
    "      top_words_csv_path,\n",
    "      predict_topic_csv_path,\n",
    "      html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e7a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1898.0500592124558"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c1692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
