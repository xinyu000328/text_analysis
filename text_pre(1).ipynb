{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c786ee0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import chardet\n",
    "import emoji\n",
    "import sys\n",
    "import unicodedata\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cadc0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670bfce5",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "#并行初始化\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6256090",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\my research\\\\dictionary\\\\dict_affect.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#载入情感词典\u001b[39;00m\n\u001b[0;32m     19\u001b[0m affect_dict_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmy research\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdictionary\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdict_affect.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 20\u001b[0m affect_dict \u001b[38;5;241m=\u001b[39m load_affect_dict(affect_dict_file)\n",
      "Cell \u001b[1;32mIn [3], line 11\u001b[0m, in \u001b[0;36mload_affect_dict\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      9\u001b[0m     m_col \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m     m_affectdict\u001b[38;5;241m.\u001b[39mappend(m_col)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m_line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[0;32m     12\u001b[0m     m_line \u001b[38;5;241m=\u001b[39m m_line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     13\u001b[0m     kwd \u001b[38;5;241m=\u001b[39m m_line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\my research\\\\dictionary\\\\dict_affect.txt'"
     ]
    }
   ],
   "source": [
    "affect_col_list = ['PA','PE','PD','PH','PG','PB','PK',\n",
    "                   'NA','NB','NJ','NH','PF','NI','NC',\n",
    "                   'NG','NE','ND','NN','NK','NL','PC',\n",
    "                  'MH','MS','MA','MD','ME',\n",
    "                  'P','N','Ne']\n",
    "def load_affect_dict(filepath):\n",
    "    m_affectdict = []\n",
    "    for m_col in affect_col_list:\n",
    "        m_col = []\n",
    "        m_affectdict.append(m_col)\n",
    "    for m_line in open(filepath,'r',encoding='utf-8').readlines():\n",
    "        m_line = m_line.strip()\n",
    "        kwd = m_line.split('\\t')[0].strip()\n",
    "        col = m_line.split('\\t')[1].strip()\n",
    "        m_affectdict[affect_col_list.index(col)].append(kwd)\n",
    "    return m_affectdict\n",
    "\n",
    "#载入情感词典\n",
    "affect_dict_file = 'D:\\my research\\dictionary\\dict_affect.txt'\n",
    "affect_dict = load_affect_dict(affect_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31202a12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stopwords为停用词list\n",
    "#stopwords = [line.strip() for line in open('stop.txt', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2da9d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#jieba分词\n",
    "df123 = pd.read_csv(r'D:\\大数据研究\\shaixuan123.csv',usecols=['content','user_name'],encoding='utf-8')\n",
    "#df.columns = ['0','content','user_name']\n",
    "#group_1 = group_1.drop(columns='0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7624334b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#def cut_word(word):\n",
    "#    cw = jieba.cut(word,cut_all=True)\n",
    "#    return list(cw)\n",
    "\n",
    "#group_1['cut_word'] = group_1['content'].apply(cut_word)\n",
    "\n",
    "#pd.Series(group_1['cut_word'].sum()).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35edf937",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#group_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49707135",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#group_1.to_csv('D:\\my research\\group\\group_1_cut.csv',encoding=\"utf-8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31192e35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>安小下同学</td>\n",
       "      <td>#上海疫情# 共克时艰，都辛苦了 2上海 &lt;U+200B&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quan-卷蕊</td>\n",
       "      <td>#上海疫情#这波疫情很厉害，各个区就这几天逐个失守，虽然相信上海速度，还是焦虑，毕竟小区封了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>正版菲儿66</td>\n",
       "      <td>#上海疫情#这次疫情怎么会这么严重？小区里有楼被封了，新冠以来第一次遇到，真的好怕！ &lt;U+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toru_梁宸</td>\n",
       "      <td>#上海疫情# 今天刚好在第十人民医院 预感苗头不对直接润了，回来几小时以后身边的人都说出事了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彩虹韩</td>\n",
       "      <td>#上海疫情#  第一次进距离感受拉线-大白-&lt;U+0001F691&gt; -志愿者-搭架子-搭桌...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_name                                            content\n",
       "0     安小下同学                      #上海疫情# 共克时艰，都辛苦了 2上海 <U+200B>\n",
       "1   quan-卷蕊  #上海疫情#这波疫情很厉害，各个区就这几天逐个失守，虽然相信上海速度，还是焦虑，毕竟小区封了...\n",
       "2    正版菲儿66  #上海疫情#这次疫情怎么会这么严重？小区里有楼被封了，新冠以来第一次遇到，真的好怕！ <U+...\n",
       "3   toru_梁宸  #上海疫情# 今天刚好在第十人民医院 预感苗头不对直接润了，回来几小时以后身边的人都说出事了...\n",
       "4       彩虹韩  #上海疫情#  第一次进距离感受拉线-大白-<U+0001F691> -志愿者-搭架子-搭桌..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practise = df123[:1000]\n",
    "practise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29571573",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name    object\n",
       "content      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practise.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf4d3fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stop_words1893(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ba59d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ASUS\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.390 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>content</th>\n",
       "      <th>cut_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>安小下同学</td>\n",
       "      <td>#上海疫情# 共克时艰，都辛苦了 2上海 &lt;U+200B&gt;</td>\n",
       "      <td># 上海 疫情 #   共克 时艰 ， 都 辛苦 了   2 上海   &lt; U + 200B &gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quan-卷蕊</td>\n",
       "      <td>#上海疫情#这波疫情很厉害，各个区就这几天逐个失守，虽然相信上海速度，还是焦虑，毕竟小区封了...</td>\n",
       "      <td># 上海 疫情 # 这波 疫情 很 厉害 ， 各个 区 就 这 几天 逐个 失守 ， 虽然 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>正版菲儿66</td>\n",
       "      <td>#上海疫情#这次疫情怎么会这么严重？小区里有楼被封了，新冠以来第一次遇到，真的好怕！ &lt;U+...</td>\n",
       "      <td># 上海 疫情 # 这次 疫情 怎么 会 这么 严重 ？ 小区 里 有 楼 被 封 了 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toru_梁宸</td>\n",
       "      <td>#上海疫情# 今天刚好在第十人民医院 预感苗头不对直接润了，回来几小时以后身边的人都说出事了...</td>\n",
       "      <td># 上海 疫情 #   今天 刚好 在 第十 人民 医院   预感 苗头 不 对 直接 润 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彩虹韩</td>\n",
       "      <td>#上海疫情#  第一次进距离感受拉线-大白-&lt;U+0001F691&gt; -志愿者-搭架子-搭桌...</td>\n",
       "      <td># 上海 疫情 #     第一次 进 距离 感受 拉线 - 大白 - &lt; U + 0001...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_name                                            content  \\\n",
       "0     安小下同学                      #上海疫情# 共克时艰，都辛苦了 2上海 <U+200B>   \n",
       "1   quan-卷蕊  #上海疫情#这波疫情很厉害，各个区就这几天逐个失守，虽然相信上海速度，还是焦虑，毕竟小区封了...   \n",
       "2    正版菲儿66  #上海疫情#这次疫情怎么会这么严重？小区里有楼被封了，新冠以来第一次遇到，真的好怕！ <U+...   \n",
       "3   toru_梁宸  #上海疫情# 今天刚好在第十人民医院 预感苗头不对直接润了，回来几小时以后身边的人都说出事了...   \n",
       "4       彩虹韩  #上海疫情#  第一次进距离感受拉线-大白-<U+0001F691> -志愿者-搭架子-搭桌...   \n",
       "\n",
       "                                         cut_content  \n",
       "0   # 上海 疫情 #   共克 时艰 ， 都 辛苦 了   2 上海   < U + 200B >  \n",
       "1  # 上海 疫情 # 这波 疫情 很 厉害 ， 各个 区 就 这 几天 逐个 失守 ， 虽然 ...  \n",
       "2  # 上海 疫情 # 这次 疫情 怎么 会 这么 严重 ？ 小区 里 有 楼 被 封 了 ， ...  \n",
       "3  # 上海 疫情 #   今天 刚好 在 第十 人民 医院   预感 苗头 不 对 直接 润 ...  \n",
       "4  # 上海 疫情 #     第一次 进 距离 感受 拉线 - 大白 - < U + 0001...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "df123=df123.copy()\n",
    "practise['cut_content'] = practise.content.apply(stop_words1893)\n",
    "practise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe807470",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       # 上海 疫情 #   共克 时艰 ， 都 辛苦 了   2 上海   < U + 200B >\n",
      "1      # 上海 疫情 # 这波 疫情 很 厉害 ， 各个 区 就 这 几天 逐个 失守 ， 虽然 ...\n",
      "2      # 上海 疫情 # 这次 疫情 怎么 会 这么 严重 ？ 小区 里 有 楼 被 封 了 ， ...\n",
      "3      # 上海 疫情 #   今天 刚好 在 第十 人民 医院   预感 苗头 不 对 直接 润 ...\n",
      "4      # 上海 疫情 #     第一次 进 距离 感受 拉线 - 大白 - < U + 0001...\n",
      "                             ...                        \n",
      "995    # 上海 疫情 # 这 发布会 不 就 大家 一起 打官腔 除了 公布 确诊 和 无症状 的...\n",
      "996                    # 上海 疫情 # 天天 在 跑 毒   < U + 200B >\n",
      "997    # 上海 疫情 # 今天 是 1 / 6394 麻 了 麻 了   2 上海   < U +...\n",
      "998    # 上海 疫情 # 喜欢 捂 嘴 是 吗 ， 喜欢 踢皮球 是 吗 ， 怎么 不去 男足 啊...\n",
      "999    昨天上午 的 核酸 结果 还 没 出来 ， 马上 又 要 开启 第二轮 核酸 ， 好 几天 ...\n",
      "Name: cut_content, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (practise['cut_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10aa44d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#创建停用词list\n",
    "def load_stopwords(filepath):\n",
    "    m_stopwords = [line.strip()for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return m_stopwords\n",
    "\n",
    "#载入停用词表\n",
    "#stop_word_file = 'D:\\my research\\dictionary\\stop_words_cn.txt'\n",
    "stop_word_file = r'C:\\Users\\ASUS\\PycharmProjects\\pythonProject1\\stop_words1893.txt'\n",
    "stopwords = load_stopwords(stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61261bae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#def find_chinese(file):\n",
    "#    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "#    chinese = re.sub(pattern, '', file)\n",
    "#    print(chinese)\n",
    " \n",
    "#def find_unchinese(file):\n",
    "#    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n",
    "#    unchinese = re.sub(pattern,\"\",file)\n",
    "#    print(unchinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9246ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stopwords[0]='!'\n",
    "stopwords.append('##')\n",
    "stopwords.append('链接')\n",
    "stopwords.append('，')\n",
    "stopwords.append('！')\n",
    "stopwords.append('？')\n",
    "new = (['网页','链接','上海','罗一舟','肖战','朱一龙','任嘉伦','邓伦','微博','视频'])\n",
    "stopwords.extend(new)\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e0d0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#str 和unicode的互转\n",
    "#coding=utf-8\n",
    "def to_unicode(unicode_or_str):\n",
    "    if isinstance(unicode_or_str,str):\n",
    "        value=unicode_or_str.decode('utf-8')\n",
    "    else:\n",
    "        value=unicode_or_str\n",
    "    return value\n",
    "def to_str(unicode_or_str):\n",
    "    value=unicode_or_str.encode('utf-8')\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57b639",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "practise['cut_content'] = to_unicode(practise.cut_content)\n",
    "stopwords = to_unicode(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9481541",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stop = stopwords # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stop:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    "\n",
    "out = []\n",
    "for line in practise['content']:\n",
    "    line_seg = seg_sentence(line) # 这里的返回值是字符串\n",
    "    out.append(line_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e70b06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "u_emoji = []\n",
    "for line in out:\n",
    "    line_u = str(line)\n",
    "    line_un = emoji.replace_emoji(line_u,'') # 这里的返回值是字符串\n",
    "    line_u = re.sub('[a-zA-Z]','',line_un) #去掉英文字母\n",
    "    line_f = re.sub(r'[0-9]+', '', line_u) #去掉数字\n",
    "    #line_ff = re.sub('([^\\u4e00-\\u9fa5])', '', line_f) #去掉除中文外全部字符\n",
    "    u_emoji.append(line_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5de11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "u_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ca7c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new = ''.join(u_emoji)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39eb8c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name = ['del']\n",
    "test=pd.DataFrame(columns=name,data=u_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6d957",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 列拼接,默认是并集\n",
    "tt=pd.concat([practise,test],axis=1)\n",
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5478fa6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputpath='D:/大数据研究/output.csv'\n",
    "tt.to_csv(outputpath,columns=['del'],index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fdce3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197c4cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=frozenset(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97947a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8939c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb407ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df, \n",
    "                       min_df = min_df, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(practise.cut_content).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628157c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_extraction(words):\n",
    "    count = []\n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(words)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "            s = words[i]\n",
    "            if(s in affect_dict[idx]):\n",
    "                affect_cnt += 1\n",
    "        \n",
    "        #计算比率\n",
    "        r_affect = 0.0\n",
    "        if (cnt_tags > 0):\n",
    "            r_affect = affect_cnt/cnt_tags\n",
    "        \n",
    "        new = str(affect_col_list[idx])+' '+str(affect_cnt)+' '+str(r_affect)\n",
    "        count.append(new) \n",
    "        idx += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "aaa = []\n",
    "for line in u_emoji:\n",
    "    line_u = str(line)\n",
    "    line_uu = line_u.split()\n",
    "    aaa.append(line_uu)\n",
    "\n",
    "new=sum(aaa,[])\n",
    "affect_cun = feature_extraction(new) # 这里的返回值是字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53735fb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "affect_cun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75732d13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df, \n",
    "                       min_df = min_df, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c9c07",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#词频统计\n",
    "#特征提取\n",
    "def feature_extraction(x_buf):\n",
    "    item = []\n",
    "    \n",
    "    #精确切分\n",
    "    m_tags = jieba.cut(x_buf, cut_all=False)\n",
    "    \n",
    "    key_wrds = []\n",
    "    #去停词\n",
    "    for s in m_tags:\n",
    "        s = s.strip()\n",
    "        # 删除停词\n",
    "        if (len(s) > 0) and (s not in stopwords):\n",
    "            key_wrds.append(s)\n",
    "            \n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(key_wrds)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "            s = key_wrds[i]\n",
    "            if(s in affect_dict[idx]):\n",
    "                affect_cnt += 1\n",
    "        \n",
    "        #计算比率\n",
    "        r_affect = 0.0\n",
    "        if (cnt_tags > 0):\n",
    "            r_affect = affect_cnt/cnt_tags\n",
    "        \n",
    "        item.append(r_affect)\n",
    "                \n",
    "        idx += 1\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c41d43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df, \n",
    "                       min_df = min_df, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bf263",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1ca2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#词频统计\n",
    "#特征提取\n",
    "def feature_extraction(x_buf):\n",
    "    item = []\n",
    "    \n",
    "    #精确切分\n",
    "    m_tags = jieba.cut(x_buf, cut_all=False)\n",
    "    \n",
    "    key_wrds = []\n",
    "    #去停词\n",
    "    for s in m_tags:\n",
    "        s = s.strip()\n",
    "        # 删除停词\n",
    "        if (len(s) > 0) and (s not in stopwords):\n",
    "            key_wrds.append(s)\n",
    "            \n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(key_wrds)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "            s = key_wrds[i]\n",
    "            if(s in affect_dict[idx]):\n",
    "                affect_cnt += 1\n",
    "        \n",
    "        #计算比率\n",
    "        r_affect = 0.0\n",
    "        if (cnt_tags > 0):\n",
    "            r_affect = affect_cnt/cnt_tags\n",
    "        \n",
    "        item.append(r_affect)\n",
    "                \n",
    "        idx += 1\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425d707",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c516594",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "u_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f1eb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#词频统计\n",
    "#特征提取\n",
    "def feature_extraction(x_buf):\n",
    "    item = []\n",
    "    \n",
    "    #精确切分\n",
    "    m_tags = jieba.cut(x_buf, cut_all=False)\n",
    "    \n",
    "    key_wrds = []\n",
    "    #去停词\n",
    "    for s in m_tags:\n",
    "        s = s.strip()\n",
    "        # 删除停词\n",
    "        if (len(s) > 0) and (s not in stopwords):\n",
    "            key_wrds.append(s)\n",
    "            \n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(key_wrds)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54722f42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#主题建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afd290",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pip install pyLDAvis==2.1.2 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd482639",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    " \n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    " \n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from gensim import corpora\n",
    "import matplotlib\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd002243",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccd1a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 构造 TF-IDF\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(tt['del'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135e6c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 特征词列表\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "# 特征词 TF-IDF 矩阵\n",
    "matrix = tf_idf.toarray()\n",
    "feature_names_df = pd.DataFrame(matrix,columns=feature_names)\n",
    "print(feature_names_df)\n",
    "feature_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb73fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 指定 lda 主题数\n",
    "n_topics = 4\n",
    "# 要输出的每个主题的前 n_top_words 个主题词数\n",
    "n_top_words = 25\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, max_iter=100,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0)\n",
    "# n_components 主题数量 learning_method更新主题的方法（online VS batch） learning_offset一个正参数，可以减轻早期迭代的负担\n",
    "#max_iter迭代的最大次数\n",
    "# 核心，给 LDA 喂生成的 TF-IDF 矩阵\n",
    "lda.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508866f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_words_data_frame(model: LatentDirichletAllocation,\n",
    "                         tf_idf_vectorizer: TfidfVectorizer,\n",
    "                         n_top_words: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出每个主题的前 n_top_words 个词\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    tf_idf_vectorizer : sklearn 的 TfidfVectorizer\n",
    "    n_top_words :前 n_top_words 个主题词\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    rows = []\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        rows.append(top_words)\n",
    "    columns = [f'topic {i+1}' for i in range(n_top_words)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_to_data_frame(model: LatentDirichletAllocation, X: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出文档主题概率分布情况\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    X : 词向量矩阵\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    # 求出给定文档的主题概率分布矩阵\n",
    "    matrix = model.transform(X)\n",
    "    columns = [f'P(topic {i+1})' for i in range(len(model.components_))]\n",
    "    df = pd.DataFrame(matrix, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d1fbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算 n_top_words 个主题词\n",
    "top_words_df = top_words_data_frame(lda, tf_idf_vectorizer, n_top_words)\n",
    "top_words_df\n",
    "# 保存 n_top_words 个主题词到 csv 文件中\n",
    "#top_words_df.to_csv(top_words_csv_path, encoding='utf-8-sig', index=None)\n",
    "\n",
    "# 转 tf_idf 为数组，以便后面使用它来对文本主题概率分布进行计算\n",
    "X = tf_idf.toarray()\n",
    "\n",
    "# 计算完毕主题概率分布情况\n",
    "predict_df = predict_to_data_frame(lda, X)\n",
    "predict_df\n",
    "# 保存文本主题概率分布到 csv 文件中\n",
    "#predict_df.to_csv(predict_topic_csv_path, encoding='utf-8-sig', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3c012",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384a88b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PATH = \"D:/大数据研究/output.csv\"\n",
    "file_object2=open(PATH,encoding = 'utf-8',errors = 'ignore').read().split('\\n')  #一行行的读取内容\n",
    "data_set=[]  #建立存储分词的列表\n",
    "for i in range(len(file_object2)):\n",
    "    result=[]\n",
    "    seg_list = file_object2[i].split()\n",
    "    for w in seg_list :  #读取每一行分词\n",
    "        result.append(w)\n",
    "    data_set.append(result)\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134a9b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data_set)  # 构建词典\n",
    "corpus = [dictionary.doc2bow(text) for text in data_set]  #表示为第几个单词出现了几次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba943d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus, num_topics=n_topics, id2word = dictionary, passes=30,random_state = 1)   #分为10个主题\n",
    "print(ldamodel.print_topics(num_topics=n_topics, num_words=15))  #每个主题输出15个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c0c70",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b926a00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#计算困惑度\n",
    "'''\n",
    "def perplexity(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=15))\n",
    "    print(ldamodel.log_perplexity(corpus))\n",
    "    return ldamodel.log_perplexity(corpus)\n",
    "    '''\n",
    "#计算coherence\n",
    "def coherence(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30,random_state = 1)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=15))\n",
    "    ldacm = CoherenceModel(model=ldamodel, texts=data_set, dictionary=dictionary, coherence='c_v')\n",
    "    print(ldacm.get_coherence())\n",
    "    return ldacm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 html 文件路径\n",
    "html_path = 'document-lda-visualization.html'\n",
    "# 使用 pyLDAvis 进行可视化\n",
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "pyLDAvis.save_html(data, html_path)\n",
    "# 清屏\n",
    "os.system('clear')\n",
    "# 浏览器打开 html 文件以查看可视化结果\n",
    "os.system(f'start {html_path}')\n",
    "\n",
    "#print('本次生成了文件：',\n",
    "#      top_words_csv_path,\n",
    "#      predict_topic_csv_path,\n",
    "#      html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "#让可视化可以在notebook内显示\n",
    "pyLDAvis.display(data)\n",
    "#%%\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd2657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975fa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94896513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
