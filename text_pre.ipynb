{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c786ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import chardet\n",
    "import emoji\n",
    "import sys\n",
    "import unicodedata\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670bfce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "#并行初始化\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f5c99",
   "metadata": {},
   "source": [
    "## copy的载入情感词典和停用词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6256090",
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_col_list = ['PA','PE','PD','PH','PG','PB','PK',\n",
    "                   'NA','NB','NJ','NH','PF','NI','NC',\n",
    "                   'NG','NE','ND','NN','NK','NL','PC',\n",
    "                  'MH','MS','MA','MD','ME',\n",
    "                  'P','N','Ne']\n",
    "def load_affect_dict(filepath):\n",
    "    m_affectdict = []\n",
    "    for m_col in affect_col_list:\n",
    "        m_col = []\n",
    "        m_affectdict.append(m_col)\n",
    "    for m_line in open(filepath,'r',encoding='utf-8').readlines():\n",
    "        m_line = m_line.strip()\n",
    "        kwd = m_line.split('\\t')[0].strip()\n",
    "        col = m_line.split('\\t')[1].strip()\n",
    "        m_affectdict[affect_col_list.index(col)].append(kwd)\n",
    "    return m_affectdict\n",
    "\n",
    "#载入情感词典\n",
    "affect_dict_file = 'D:\\my research\\dictionary\\dict_affect.txt'\n",
    "affect_dict = load_affect_dict(affect_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f5e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建停用词list\n",
    "def load_stopwords(filepath):\n",
    "    m_stopwords = [line.strip()for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return m_stopwords\n",
    "\n",
    "#载入停用词表\n",
    "stop_word_file = 'D:\\my research\\dictionary\\stop_words_cn.txt'\n",
    "stopwords = load_stopwords(stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ba2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords为停用词list\n",
    "#stopwords = [line.strip() for line in open('stop.txt', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b13c94",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded5cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "group_1 = pd.read_csv('D:\\my research\\group\\group_1.csv',encoding='utf-8')\n",
    "group_1.columns = ['0','content','date','time','user_id']\n",
    "group_1 = group_1.drop(columns='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648a0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cut_word(word):\n",
    "#    cw = jieba.cut(word,cut_all=True)\n",
    "#    return list(cw)\n",
    "\n",
    "#group_1['cut_word'] = group_1['content'].apply(cut_word)\n",
    "\n",
    "#pd.Series(group_1['cut_word'].sum()).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11694f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994bfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_1.to_csv('D:\\my research\\group\\group_1_cut.csv',encoding=\"utf-8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cdc4f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022-04-24 08:33:43</td>\n",
       "      <td>1000074972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2022-04-23 05:46:14</td>\n",
       "      <td>1000074972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 11:47:31</td>\n",
       "      <td>1000074972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 09:41:33</td>\n",
       "      <td>1000074972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21 08:54:29</td>\n",
       "      <td>1000074972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  #上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...  2022-04-24   \n",
       "1  #上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...  2022-04-23   \n",
       "2  #上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...  2022-04-22   \n",
       "3        #上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。   2022-04-22   \n",
       "4  明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...  2022-04-21   \n",
       "\n",
       "                  time     user_id  \n",
       "0  2022-04-24 08:33:43  1000074972  \n",
       "1  2022-04-23 05:46:14  1000074972  \n",
       "2  2022-04-22 11:47:31  1000074972  \n",
       "3  2022-04-22 09:41:33  1000074972  \n",
       "4  2022-04-21 08:54:29  1000074972  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practise = group_1[0:100]\n",
    "practise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e56f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    object\n",
       "date       object\n",
       "time       object\n",
       "user_id     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practise.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0a2de",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e4c4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c7cfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.637 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp/ipykernel_11392/1728287109.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  practise['cut_content'] = practise.content.apply(chinese_word_cut)\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "practise['cut_content'] = practise.content.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e55cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cut_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022-04-24 08:33:43</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2022-04-23 05:46:14</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 11:47:31</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 09:41:33</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21 08:54:29</td>\n",
       "      <td>1000074972</td>\n",
       "      <td>明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  #上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...  2022-04-24   \n",
       "1  #上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...  2022-04-23   \n",
       "2  #上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...  2022-04-22   \n",
       "3        #上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。   2022-04-22   \n",
       "4  明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...  2022-04-21   \n",
       "\n",
       "                  time     user_id  \\\n",
       "0  2022-04-24 08:33:43  1000074972   \n",
       "1  2022-04-23 05:46:14  1000074972   \n",
       "2  2022-04-22 11:47:31  1000074972   \n",
       "3  2022-04-22 09:41:33  1000074972   \n",
       "4  2022-04-21 08:54:29  1000074972   \n",
       "\n",
       "                                         cut_content  \n",
       "0  # 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...  \n",
       "1  # 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...  \n",
       "2  # 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...  \n",
       "3  # 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...  \n",
       "4  明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def find_chinese(file):\n",
    "#    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "#    chinese = re.sub(pattern, '', file)\n",
    "#    print(chinese)\n",
    " \n",
    "#def find_unchinese(file):\n",
    "#    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n",
    "#    unchinese = re.sub(pattern,\"\",file)\n",
    "#    print(unchinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc69fc8",
   "metadata": {},
   "source": [
    "## 修改停用词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc2736ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ad5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords[0]='!'\n",
    "stopwords.append('##')\n",
    "stopwords.append('链接')\n",
    "new = (['网页','链接','上海','罗一舟','肖战','朱一龙','任嘉伦','邓伦','微博','视频'])\n",
    "stopwords.extend(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df4b35d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '......',\n",
       " '...................',\n",
       " './',\n",
       " '.一',\n",
       " '.数',\n",
       " '.日',\n",
       " '/',\n",
       " '//',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '://',\n",
       " '::',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '>>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'Lex',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'exp',\n",
       " 'sub',\n",
       " 'sup',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '~~~~',\n",
       " '·',\n",
       " '×',\n",
       " '×××',\n",
       " 'Δ',\n",
       " 'Ψ',\n",
       " 'γ',\n",
       " 'μ',\n",
       " 'φ',\n",
       " 'φ．',\n",
       " 'В',\n",
       " '—',\n",
       " '——',\n",
       " '———',\n",
       " '‘',\n",
       " '’',\n",
       " '’‘',\n",
       " '“',\n",
       " '”',\n",
       " '”，',\n",
       " '…',\n",
       " '……',\n",
       " '…………………………………………………③',\n",
       " '′∈',\n",
       " '′｜',\n",
       " '℃',\n",
       " 'Ⅲ',\n",
       " '↑',\n",
       " '→',\n",
       " '∈［',\n",
       " '∪φ∈',\n",
       " '≈',\n",
       " '①',\n",
       " '②',\n",
       " '②ｃ',\n",
       " '③',\n",
       " '③］',\n",
       " '④',\n",
       " '⑤',\n",
       " '⑥',\n",
       " '⑦',\n",
       " '⑧',\n",
       " '⑨',\n",
       " '⑩',\n",
       " '──',\n",
       " '■',\n",
       " '▲',\n",
       " '',\n",
       " '、',\n",
       " '。',\n",
       " '〈',\n",
       " '〉',\n",
       " '《',\n",
       " '》',\n",
       " '》），',\n",
       " '」',\n",
       " '『',\n",
       " '』',\n",
       " '【',\n",
       " '】',\n",
       " '〔',\n",
       " '〕',\n",
       " '〕〔',\n",
       " '㈧',\n",
       " '一',\n",
       " '一.',\n",
       " '一一',\n",
       " '一下',\n",
       " '一个',\n",
       " '一些',\n",
       " '一何',\n",
       " '一切',\n",
       " '一则',\n",
       " '一则通过',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一番',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一转眼',\n",
       " '一边',\n",
       " '一面',\n",
       " '七',\n",
       " '万一',\n",
       " '三',\n",
       " '三天两头',\n",
       " '三番两次',\n",
       " '三番五次',\n",
       " '上',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不',\n",
       " '不一',\n",
       " '不下',\n",
       " '不久',\n",
       " '不了',\n",
       " '不亦乐乎',\n",
       " '不仅',\n",
       " '不仅...而且',\n",
       " '不仅仅',\n",
       " '不仅仅是',\n",
       " '不会',\n",
       " '不但',\n",
       " '不但...而且',\n",
       " '不光',\n",
       " '不免',\n",
       " '不再',\n",
       " '不力',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不可开交',\n",
       " '不可抗拒',\n",
       " '不同',\n",
       " '不外',\n",
       " '不外乎',\n",
       " '不够',\n",
       " '不大',\n",
       " '不如',\n",
       " '不妨',\n",
       " '不定',\n",
       " '不对',\n",
       " '不少',\n",
       " '不尽',\n",
       " '不尽然',\n",
       " '不巧',\n",
       " '不已',\n",
       " '不常',\n",
       " '不得',\n",
       " '不得不',\n",
       " '不得了',\n",
       " '不得已',\n",
       " '不必',\n",
       " '不怎么',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不择手段',\n",
       " '不敢',\n",
       " '不料',\n",
       " '不断',\n",
       " '不日',\n",
       " '不时',\n",
       " '不是',\n",
       " '不曾',\n",
       " '不止',\n",
       " '不止一次',\n",
       " '不比',\n",
       " '不消',\n",
       " '不满',\n",
       " '不然',\n",
       " '不然的话',\n",
       " '不特',\n",
       " '不独',\n",
       " '不由得',\n",
       " '不知不觉',\n",
       " '不管',\n",
       " '不管怎样',\n",
       " '不经意',\n",
       " '不胜',\n",
       " '不能',\n",
       " '不能不',\n",
       " '不至于',\n",
       " '不若',\n",
       " '不要',\n",
       " '不论',\n",
       " '不起',\n",
       " '不足',\n",
       " '不过',\n",
       " '不迭',\n",
       " '不问',\n",
       " '不限',\n",
       " '与',\n",
       " '与其',\n",
       " '与其说',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '且不说',\n",
       " '且说',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '串行',\n",
       " '临',\n",
       " '临到',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为止',\n",
       " '为此',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举凡',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '乃至于',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乒',\n",
       " '乘',\n",
       " '乘势',\n",
       " '乘机',\n",
       " '乘胜',\n",
       " '乘虚',\n",
       " '乘隙',\n",
       " '九',\n",
       " '也',\n",
       " '也好',\n",
       " '也就是说',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '二',\n",
       " '二来',\n",
       " '二话不说',\n",
       " '二话没说',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '云尔',\n",
       " '互',\n",
       " '互相',\n",
       " '五',\n",
       " '些',\n",
       " '交口',\n",
       " '亦',\n",
       " '产生',\n",
       " '亲口',\n",
       " '亲手',\n",
       " '亲眼',\n",
       " '亲自',\n",
       " '亲身',\n",
       " '人',\n",
       " '人人',\n",
       " '人们',\n",
       " '人家',\n",
       " '人民',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '仅',\n",
       " '仅仅',\n",
       " '今',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '介于',\n",
       " '仍',\n",
       " '仍旧',\n",
       " '仍然',\n",
       " '从',\n",
       " '从不',\n",
       " '从严',\n",
       " '从中',\n",
       " '从事',\n",
       " '从今以后',\n",
       " '从优',\n",
       " '从古到今',\n",
       " '从古至今',\n",
       " '从头',\n",
       " '从宽',\n",
       " '从小',\n",
       " '从新',\n",
       " '从无到有',\n",
       " '从早到晚',\n",
       " '从未',\n",
       " '从来',\n",
       " '从此',\n",
       " '从此以后',\n",
       " '从而',\n",
       " '从轻',\n",
       " '从速',\n",
       " '从重',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他是',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以故',\n",
       " '以期',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伙同',\n",
       " '会',\n",
       " '伟大',\n",
       " '传',\n",
       " '传说',\n",
       " '传闻',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但凡',\n",
       " '但愿',\n",
       " '但是',\n",
       " '何',\n",
       " '何乐而不为',\n",
       " '何以',\n",
       " '何况',\n",
       " '何处',\n",
       " '何妨',\n",
       " '何尝',\n",
       " '何必',\n",
       " '何时',\n",
       " '何止',\n",
       " '何苦',\n",
       " '何须',\n",
       " '余外',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你是',\n",
       " '你的',\n",
       " '使',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依据',\n",
       " '依照',\n",
       " '依靠',\n",
       " '便',\n",
       " '便于',\n",
       " '促进',\n",
       " '保持',\n",
       " '保管',\n",
       " '保险',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倍加',\n",
       " '倍感',\n",
       " '倒不如',\n",
       " '倒不如说',\n",
       " '倒是',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '借',\n",
       " '借以',\n",
       " '借此',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '偏偏',\n",
       " '做到',\n",
       " '偶尔',\n",
       " '偶而',\n",
       " '傥然',\n",
       " '像',\n",
       " '儿',\n",
       " '允许',\n",
       " '元／吨',\n",
       " '充其极',\n",
       " '充其量',\n",
       " '充分',\n",
       " '先不先',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '光',\n",
       " '光是',\n",
       " '全体',\n",
       " '全力',\n",
       " '全年',\n",
       " '全然',\n",
       " '全身心',\n",
       " '全部',\n",
       " '全都',\n",
       " '全面',\n",
       " '八',\n",
       " '八成',\n",
       " '公然',\n",
       " '六',\n",
       " '兮',\n",
       " '共',\n",
       " '共同',\n",
       " '共总',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其后',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体来说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '兼之',\n",
       " '内',\n",
       " '再',\n",
       " '再其次',\n",
       " '再则',\n",
       " '再有',\n",
       " '再次',\n",
       " '再者',\n",
       " '再者说',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决不',\n",
       " '决定',\n",
       " '决非',\n",
       " '况且',\n",
       " '准备',\n",
       " '凑巧',\n",
       " '凝神',\n",
       " '几',\n",
       " '几乎',\n",
       " '几度',\n",
       " '几时',\n",
       " '几番',\n",
       " '几经',\n",
       " '凡',\n",
       " '凡是',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出',\n",
       " '出于',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '分头',\n",
       " '分期',\n",
       " '分期分批',\n",
       " '切',\n",
       " '切不可',\n",
       " '切切',\n",
       " '切勿',\n",
       " '切莫',\n",
       " '则',\n",
       " '则甚',\n",
       " '刚',\n",
       " '刚好',\n",
       " '刚巧',\n",
       " '刚才',\n",
       " '初',\n",
       " '别',\n",
       " '别人',\n",
       " '别处',\n",
       " '别是',\n",
       " '别的',\n",
       " '别管',\n",
       " '别说',\n",
       " '到',\n",
       " '到了儿',\n",
       " '到处',\n",
       " '到头',\n",
       " '到头来',\n",
       " '到底',\n",
       " '到目前为止',\n",
       " '前后',\n",
       " '前此',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加上',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '动不动',\n",
       " '动辄',\n",
       " '勃然',\n",
       " '匆匆',\n",
       " '十分',\n",
       " '千',\n",
       " '千万',\n",
       " '千万千万',\n",
       " '半',\n",
       " '单',\n",
       " '单单',\n",
       " '单纯',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即刻',\n",
       " '即如',\n",
       " '即将',\n",
       " '即或',\n",
       " '即是说',\n",
       " '即若',\n",
       " '却',\n",
       " '却不',\n",
       " '历',\n",
       " '原来',\n",
       " '去',\n",
       " '又',\n",
       " '又及',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反之亦然',\n",
       " '反之则',\n",
       " '反倒',\n",
       " '反倒是',\n",
       " '反应',\n",
       " '反手',\n",
       " '反映',\n",
       " '反而',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '取道',\n",
       " '受到',\n",
       " '变成',\n",
       " '古来',\n",
       " '另',\n",
       " '另一个',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '另悉',\n",
       " '另方面',\n",
       " '另行',\n",
       " '只',\n",
       " '只当',\n",
       " '只怕',\n",
       " '只是',\n",
       " '只有',\n",
       " '只消',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '叮当',\n",
       " '可',\n",
       " '可以',\n",
       " '可好',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各式',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后',\n",
       " '后来',\n",
       " '后者',\n",
       " '后面',\n",
       " '向',\n",
       " '向使',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呆呆地',\n",
       " '呐',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呵呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '呼啦',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咧',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哗啦',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '唯有',\n",
       " '啊',\n",
       " '啊呀',\n",
       " '啊哈',\n",
       " '啊哟',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '啷当',\n",
       " '喀',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '喽',\n",
       " '嗡',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '嘿嘿',\n",
       " '四',\n",
       " '因',\n",
       " '因为',\n",
       " '因了',\n",
       " '因此',\n",
       " '因着',\n",
       " '因而',\n",
       " '固',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '在于',\n",
       " '地',\n",
       " '均',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基于',\n",
       " '基本',\n",
       " '基本上',\n",
       " '处在',\n",
       " '处处',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多么',\n",
       " '多亏',\n",
       " '多多',\n",
       " '多多少少',\n",
       " '多多益善',\n",
       " '多少',\n",
       " '多年前',\n",
       " '多年来',\n",
       " '多数',\n",
       " '多次',\n",
       " '够瞧的',\n",
       " '大',\n",
       " '大不了',\n",
       " '大举',\n",
       " '大事',\n",
       " '大体',\n",
       " '大体上',\n",
       " '大凡',\n",
       " '大力',\n",
       " '大多',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大张旗鼓',\n",
       " '大批',\n",
       " '大抵',\n",
       " '大概',\n",
       " '大略',\n",
       " '大约',\n",
       " '大致',\n",
       " '大都',\n",
       " '大量',\n",
       " '大面儿上',\n",
       " '失去',\n",
       " '奇',\n",
       " '奈',\n",
       " '奋勇',\n",
       " '她',\n",
       " '她们',\n",
       " '她是',\n",
       " '她的',\n",
       " '好',\n",
       " '好在',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如今',\n",
       " '如何',\n",
       " '如其',\n",
       " '如前所述',\n",
       " '如同',\n",
       " '如常',\n",
       " '如是',\n",
       " '如期',\n",
       " '如果',\n",
       " '如次',\n",
       " '如此',\n",
       " '如此等等',\n",
       " '如若',\n",
       " '始而',\n",
       " '姑且',\n",
       " '存在',\n",
       " '存心',\n",
       " '孰料',\n",
       " '孰知',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它是',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '定',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '对待',\n",
       " '对方',\n",
       " '对比',\n",
       " '将',\n",
       " '将才',\n",
       " '将要',\n",
       " '将近',\n",
       " '小',\n",
       " '少数',\n",
       " '尔',\n",
       " '尔后',\n",
       " '尔尔',\n",
       " '尔等',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就地',\n",
       " '就是',\n",
       " '就是了',\n",
       " '就是说',\n",
       " '就此',\n",
       " '就算',\n",
       " '就要',\n",
       " '尽',\n",
       " '尽可能',\n",
       " '尽如人意',\n",
       " '尽心尽力',\n",
       " '尽心竭力',\n",
       " '尽快',\n",
       " '尽早',\n",
       " '尽然',\n",
       " '尽管',\n",
       " '尽管如此',\n",
       " '尽量',\n",
       " '局外',\n",
       " '居然',\n",
       " '届时',\n",
       " '属于',\n",
       " '屡',\n",
       " '屡屡',\n",
       " '屡次',\n",
       " '屡次三番',\n",
       " '岂',\n",
       " '岂但',\n",
       " '岂止',\n",
       " '岂非',\n",
       " '川流不息',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '差一点',\n",
       " '差不多',\n",
       " '己',\n",
       " '已',\n",
       " '已矣',\n",
       " '已经',\n",
       " '巴',\n",
       " '巴巴',\n",
       " '带',\n",
       " '帮助',\n",
       " '常',\n",
       " '常常',\n",
       " '常言说',\n",
       " '常言说得好',\n",
       " '常言道',\n",
       " '平素',\n",
       " '年复一年',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并排',\n",
       " '并无',\n",
       " '并没',\n",
       " '并没有',\n",
       " '并肩',\n",
       " '并非',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " '应该',\n",
       " '庶乎',\n",
       " '庶几',\n",
       " '开外',\n",
       " '开始',\n",
       " '开展',\n",
       " '引起',\n",
       " '弗',\n",
       " '弹指之间',\n",
       " '强烈',\n",
       " '强调',\n",
       " '归',\n",
       " '归根到底',\n",
       " '归根结底',\n",
       " '归齐',\n",
       " '当',\n",
       " '当下',\n",
       " '当中',\n",
       " '当儿',\n",
       " '当前',\n",
       " '当即',\n",
       " '当口儿',\n",
       " '当地',\n",
       " '当场',\n",
       " '当头',\n",
       " '当庭',\n",
       " '当时',\n",
       " '当然',\n",
       " '当真',\n",
       " '当着',\n",
       " '形成',\n",
       " '彻夜',\n",
       " '彻底',\n",
       " '彼',\n",
       " '彼时',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd702f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "647962ea",
   "metadata": {},
   "source": [
    "## 尝试转换unicode 但是失败的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a78a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#str 和unicode的互转\n",
    "#coding=utf-8\n",
    "def to_unicode(unicode_or_str):\n",
    "    if isinstance(unicode_or_str,str):\n",
    "        value=unicode_or_str.decode('utf-8')\n",
    "    else:\n",
    "        value=unicode_or_str\n",
    "    return value\n",
    "def to_str(unicode_or_str):\n",
    "    value=unicode_or_str.encode('utf-8')\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46a31759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp/ipykernel_11392/219021477.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  practise['cut_content'] = to_unicode(practise.cut_content)\n"
     ]
    }
   ],
   "source": [
    "practise['cut_content'] = to_unicode(practise.cut_content)\n",
    "stopwords = to_unicode(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd524c8",
   "metadata": {},
   "source": [
    "## 对数据框内文本进行分词加去除停用词，通过定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7e5483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stop = stopwords # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stop:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    "\n",
    "out = []\n",
    "for line in practise['content']:\n",
    "    line_seg = seg_sentence(line) # 这里的返回值是字符串\n",
    "    out.append(line_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c58f99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_emoji = []\n",
    "for line in out:\n",
    "    line_u = str(line)\n",
    "    line_un = emoji.replace_emoji(line_u,'') # 这里的返回值是字符串\n",
    "    line_u = re.sub('[a-zA-Z]','',line_un) #去掉英文字母\n",
    "    line_f = re.sub(r'[0-9]+', '', line_u) #去掉数字\n",
    "    #line_ff = re.sub('([^\\u4e00-\\u9fa5])', '', line_f) #去掉除中文外全部字符\n",
    "    u_emoji.append(line_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d252b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮 开启 滞留 人员 陆续 返乡 安徽 江苏 河南 返乡 人员 ',\n",
       " '疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 一条 汉奸 真的 太 抗战 那会 太 敏感 我乎 审核 说 一条 有人 发国难财 能量 不小 ',\n",
       " '疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼一闭 当作 事情 发生 岁月 静好 叫唤 ',\n",
       " '民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区 ',\n",
       " '明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 舟   中国新闻社 ',\n",
       " '疫情 民生 物资   黑市     ',\n",
       " '民生 疫情   外卖 骑手 好自为之 骑手 代购 袋 泡面  鸡蛋 收取  元 警方 调查 惊人 发现 最新消息 证实 赵某 因涉嫌 非法经营 罪 长宁 警方 依法 强制措施 案件 进一步 侦办 中 ',\n",
       " '民生 疫情   连花清 瘟 牛 逼 连花清 瘟 站台  收钱 办事 钱能 挣 一大批 粉丝 数十万 数百万  金  纷纷 连花清 瘟 站台 证实  收钱 连花清 瘟 站台 事实   ',\n",
       " '疫情 民生   救护车 谎称 进 小区 救人 买  条烟 业主 震怒 报 次警 近日 救护车 谎称 进 小区 救人 实则 买烟 新闻 引发 关注 ',\n",
       " '疫情 民生   跑腿 代购 牛 逼   ',\n",
       " '疫情 民生 刚刚 明白   网传 真的 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚 ',\n",
       " '疫情   核酸 检测 强烈建议 专业 事要 专业 人来 做 楼 里  未 拉走 居委会 老先生 老太太 志愿者 包括 毛估估 漏洞百出   ',\n",
       " '疫情 民生 有钱人 玩 事关 老百姓 民生 呼吁 政府部门 管管   ',\n",
       " '疫情 民生   不算 太贵 老板 还算 良心 关键 至少 送货 只会 送 烂 菜 枯叶 要强   ',\n",
       " '疫情 民生 抗疫 物资   这是 真的 话 抓 枪毙     ',\n",
       " '疫情 民生   民间 自发 团购 能力 配货 送货 政府 组织 商委   市场 管理局   ',\n",
       " '疫情   基层干部 太 当回事 能力 强 应付 上级 忽悠 下级 本事 一大堆 资料 一套 一套 关键 不行 小区 志愿者 义工 值得反思   ',\n",
       " '疫情   不要脸 东西 胡说八道   ',\n",
       " '疫情 基层 一线 干部 累 趴 确实 好多 居民 需求 人理 月 号 核酸 阳性 一轮 结束 拉走 盼望 政府  转 疾控   普陀 桃浦   宣克炅   江 丄 孤 舟   ',\n",
       " '咋啦 核酸 阳性反应 居家 隔离 普陀 桃浦 疾控   ',\n",
       " '这是 在建 方舱 施工 宣克炅   嘉定 公安   嘉定 安监   华盛 国际 商务 花园 北区 ',\n",
       " '疫情 大白菜  块钱 一颗 青菜  块钱 一斤 高估 承压 能力 月  日 国务院 联防 联控 机制 督查组 沪 防疫 方式 加速 转轨 疫情 世外桃源 居民 生计 疫情 导致 经济 压力 打击 ',\n",
       " '前两天 想 说 江桥 菜市场 封控 运菜 货车 周边 停留 自律   阳光 威尼斯 提香 湾 ',\n",
       " '防疫   不到 小时 连续 两份 通知 防控 基层 一线 混乱 镇 疫情 防控 指挥部 指示 阳光 一委  区 封控  区 解封 时间 待镇 疫情 防控 指挥部 指示 一刻 咒术 回战 冒个 泡 ',\n",
       " '百 果园 趁火打劫 百 果园 ',\n",
       " '党校 教授 求教 再驳 党校 教授 降美弃 俄     文 司马南     俄罗斯 倒下 中国 西方 围剿 司马南 百度 动态 发表 新 动态 ',\n",
       " '六院 疫情 舆论 焦点 网上 该院 发生 院内 感染 院方 负有 隐瞒 消息 处置 胡锡进 百度 动态 发表 新 动态 ',\n",
       " ' 星期天 妈妈 办公 ',\n",
       " '一周 三顿  感谢 邻居 投 冰淇淋  ',\n",
       " '居家 一个月 焦 大哥 皮肤 不行 小可爱 做 核酸 乖到 不行 ',\n",
       " '太爱 吃 三明治  烤鸡 腿  蔬菜 饼  疫情 三顿 厨娘 可惜 半个 月 好多 拍照 好好 记录 一顿 ',\n",
       " '一刻   周五  我来 干扰 妈妈 居家 办公 ',\n",
       " '真会 挑 下雨天   睡懒觉 感觉 孩子 中 长大 ',\n",
       " '体重  想 时隔 十几年 是因为 疫情 生个 孩子 瘦 内心 开心 回奶 瘦 慢慢 吃 断奶 瘦 ',\n",
       " '妈妈 发来 航拍 照片 爸爸 弄 泉水 源 太美 太想 回去 期待 国庆 回老家 ',\n",
       " '卷毛  陪 妈妈 居家 办公  可惜 天气    只能 窗台 玩 ',\n",
       " '何德何能 楼里 医院 邻居  有盒 马 员工 邻居 幸福 好好 生活 好好 吃饭 健健康康 防护  疫情 早点 结束   ',\n",
       " '健康 平安  疫情 早日 结束 ',\n",
       " '浦西 自由 明天 足不出户 保佑 平安 顺遂 健康 疫情 无情 邻里 有情 感恩   愿 孩子 爸爸 永远 健康 快乐 爱    ',\n",
       " '家门 美景     ',\n",
       " '上班  节后 健健康康 平平安安 加油      ',\n",
       " '三个 月 变化 孩子 肉眼 一圈 身高  肉眼 瘦 一圈 ',\n",
       " '沉浸 式 开箱 开箱   月  法棍 终于 犒劳 一年 真的 涨价 前入 幸福 买到 赚     ',\n",
       " '聂总说 国内 喝咖啡 奶茶 最终 喝牛奶  哈哈哈哈 解封 回归 工作 聂总投     ',\n",
       " '油 加满   一杯 抹 茶   出发 上班   上班 幸福 事情       ',\n",
       " '日上 真的 太给力 凌晨 下单 下午 不敢相信 这是 疫情 顺丰 快递 实名  日上 弥补 两年 国门 囤货 遗憾 压根 不用 囤 快完了 买 行 代购 两年 省  娇韵诗   日上 免税店 ',\n",
       " '决赛圈 出门 希望 健健康康 ',\n",
       " '老板 同事 太好了 心疼 家里 孩子 居家 办公 感恩   加油  希望 决赛圈 健健康康 疫情 居家 办公 ',\n",
       " '疫情 不易 楼下 新开  门店 支持  喝 咖啡 来个 抹 茶 铁       长阳 创谷 ',\n",
       " '一周 喝 两杯  部门 同事 离职 请 喝茶 饮 开心 朋友 天蝎座   辣妈 ',\n",
       " '开心 美丽 三月 十年 前买 第一辆 车 时间 验车  终于 一辆        金桥 ',\n",
       " '张老师   爱   女神 节 快乐   傩 晃晃 ',\n",
       " '老板 变 去年 请 喝 奶茶 喝 两杯 过分 ',\n",
       " '快乐 可爱 善良 邻居 疫情   ',\n",
       " '苹果 花园 团购    救救 孩子 封控 多天 求个 谱 苹果 花园 团购 联系方式 做 团长 坐标 青浦   ',\n",
       " '京东 快递   说 快递 解封 前 收到 希望 京东 早日 解控 早日 派送   ',\n",
       " '物资   物资 发了 感谢 内心  物资 小区 实力   ',\n",
       " '太 适合   ',\n",
       " '周四 快乐   ',\n",
       " '隔离 在家 第四天 三顿 饭 缩减 两顿 不解 封 只能 一顿 抗击 疫情 疫情   ',\n",
       " '我司 男生 想 变 女生     谢谢 宠爱   ',\n",
       " '公司  节 礼物  拿捏   ',\n",
       " '周一 惊喜 感谢 领导 女神 节 仪式 感   ',\n",
       " '徐汇 今晚 载入史册 ',\n",
       " '腾讯 成长 守护 平台 恶意 绑定 未成年人 账户 ',\n",
       " '疫情   正 能量 天使 守护星  ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫                  ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   哇塞 小区 收到  玖 少年 团肖战    工作室   影音 官微   捐赠 礼包 忙碌 亮点 感谢   ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   砖家 抗疫 ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫 ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   硬 隔离 火宅 新冠 死法 都行 ',\n",
       " '疫情   演员 成本 高 网友 扒 新闻   ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   众人 拾柴 ',\n",
       " '疫情 仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫    屯菜    ',\n",
       " '仓鼠 妞妞 端午 水彩 押花 花草 家有 糖 猫    ',\n",
       " '仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫   夜奔    ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   流传 一段  多分钟 录音 一位 男性 市民 疾控中心 女 领导 对话   这位 男士 父母 酒店 隔离 前天 做 核酸 健康 云上 阴性 接到 疾控中心 电话 说 阳性 做   专家 这位 女 领导 太极 说出 心声   疾控 健康 云 医疗机构 各自为政 混乱   家里 老人 方舱 医院 居家 隔离 观察 无症状 休息 三五天    拉 老人 强制 隔离 出示 阳性 证明 概率   有用 知识 无症状 轻症 居家 隔离 更优 选择 病 特别 治疗 对付 感冒 扛过去   弄 方舱 医院 条件 很差 治疗 不利于 病人 康复     专业 人员 发现 轻症 无症 转走 家里 隔离 提  次 有人 听过 没人 听   逼 疯 专业 机构 逼 疯 专业 人员 说 没人 听 病 政治性 疾病   录音 传播 直白 话语 说出 共识 有人 评论 里 说 领导 危险 说 真话 做 人事 希望 女 领导 越来越 ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫   仓鼠 喝奶    ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫    ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫 ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫    ',\n",
       " '妞妞 端午 花草 水彩 押花   小知 女娃娃    ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫   吾 家 女 初长成 ',\n",
       " '真他妈 烦透了   真的 垃圾 ',\n",
       " '疫情   物价 离谱 ',\n",
       " '春天 完 哭 失眠 久久 平复 ',\n",
       " '早  晚 后续 脸 过敏   眼下 脱皮 过敏 地方 一夜之间 长出 皱纹 真的 会谢   无良 商家 营销 号  ',\n",
       " '天天 辟谣 谣言 验证 事实 ',\n",
       " '真特 脑残 ',\n",
       " '一位 同事 喜提  天 组 疫情 ',\n",
       " '新增  例 本土 确诊  例 无症状 日增 两千多 影响 上班 ',\n",
       " '新增   影响 上班 地铁 依旧 人挤 工人 可太 卑微 ',\n",
       " '晚上  登陆 微信 吓人 ',\n",
       " '防疫 措施 无语 ',\n",
       " '社会 毒打   终于 悟出 道理 找对象   帅不帅 一点   搞钱   有钱 第一   丑 丑   帅不帅   看多 人烦 ',\n",
       " '想 辞职 第一天 ',\n",
       " '新 鞋子 哐 哐响 新 裙子 大步 走路   这才 周一   犯傻 ',\n",
       " '人世间 细节 处 锅包肉 ',\n",
       " 'の 天气 阴晴不定 有时候 像是 漫画 里 走 の 天气 有时候 狂风骤雨 爱 の 季节 太 可惜 晚安   愿 解封 浪 先睡 敬 晚安    明天 卖菜 の ',\n",
       " '一刻 宝宝 少女 心 天气 天意   啥时候 个头 赶紧 倒闭 叭叭 叭 求求 ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6422625",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['del']\n",
    "test=pd.DataFrame(columns=name,data=u_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6681a24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cut_content</th>\n",
       "      <th>del</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022-04-24 08:33:43</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...</td>\n",
       "      <td>疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...</td>\n",
       "      <td>2022-04-23</td>\n",
       "      <td>2022-04-23 05:46:14</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...</td>\n",
       "      <td>疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 11:47:31</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...</td>\n",
       "      <td>疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>2022-04-22 09:41:33</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...</td>\n",
       "      <td>民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>2022-04-21 08:54:29</td>\n",
       "      <td>1000074972</td>\n",
       "      <td>明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...</td>\n",
       "      <td>明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  #上海疫情##上海民生##就这一刻##返乡# 上海返乡潮开启，安徽、江苏和河南人最多，隔离和...  2022-04-24   \n",
       "1  #上海疫情##上海民生##上海现状# .上海的现状为什么会变成这样我忍不住地继续瞎想外行领导...  2022-04-23   \n",
       "2  #上海疫情##上海民生#      无间道，罗生门，可能本来就是人生如戏吧，是我太认真了。算...  2022-04-22   \n",
       "3        #上海民生##上海疫情##上海团长# 还是你牛逼呀，团长。开个车照送，还是封控小区。   2022-04-22   \n",
       "4  明康汇的采购员你成功地把你们老板一个月来的光辉形象給黑了。@明康汇 @市场监管 @江丄孤舟 ...  2022-04-21   \n",
       "\n",
       "                  time     user_id  \\\n",
       "0  2022-04-24 08:33:43  1000074972   \n",
       "1  2022-04-23 05:46:14  1000074972   \n",
       "2  2022-04-22 11:47:31  1000074972   \n",
       "3  2022-04-22 09:41:33  1000074972   \n",
       "4  2022-04-21 08:54:29  1000074972   \n",
       "\n",
       "                                         cut_content  \\\n",
       "0  # 上海 疫情 ## 上海 民生 ## 就 这 一刻 ## 返乡 #   上海 返乡 潮 开...   \n",
       "1  # 上海 疫情 ## 上海 民生 ## 上海 现状 #   . 上海 的 现状 为什么 会 ...   \n",
       "2  # 上海 疫情 ## 上海 民生 #             无间道 ， 罗生门 ， 可能 ...   \n",
       "3  # 上海 民生 ## 上海 疫情 ## 上海 团长 #   还是 你 牛 逼 呀 ， 团长 ...   \n",
       "4  明康汇 的 采购员 你 成功 地 把 你们 老板 一个月 来 的 光辉 形象 給黑 了 。 ...   \n",
       "\n",
       "                                                 del  \n",
       "0  疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮...  \n",
       "1  疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 ...  \n",
       "2  疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼...  \n",
       "3                   民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区   \n",
       "4  明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 列拼接,默认是并集\n",
    "tt = pd.concat([practise,test],axis=1)\n",
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3f7d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c39cfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def filter_emoji(desstr, restr=''):\n",
    "    # 过滤表情\n",
    "#    try:\n",
    "        #co = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "#        co = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "#    except re.error:\n",
    "#        co = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#    return co.sub(restr, desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "671b772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from emoji import get_emoji_regexp\n",
    "#un_emoji = []\n",
    "#for line in tt['del']:\n",
    "#    line_un = filter_emoji(line) # 这里的返回值是字符串\n",
    "#    un_emoji.append(line_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ab996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def give_emoji_free_text(text):\n",
    "#    allchars = [str for str in text.decode('utf-8')]\n",
    "#    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "#    clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "#    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "025dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uu_emoji = []\n",
    "#for line in tt['del']:\n",
    "#    line_un = emoji.demojize(line)# 这里的返回值是字符串\n",
    "#    print(line_un)\n",
    "#    line_u = emoji.emojize(line_un)\n",
    "#    print(line_u)\n",
    "    #uu_emoji.append(line_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de70e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = emoji.replace_emoji(u'A 🏌️‍♀️ is eating a 🥐','')\n",
    "#print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c90de",
   "metadata": {},
   "source": [
    "## copy 清华 提取关键词及其词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80311c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一个月</th>\n",
       "      <th>一位</th>\n",
       "      <th>一刻</th>\n",
       "      <th>一周</th>\n",
       "      <th>一圈</th>\n",
       "      <th>一夜之间</th>\n",
       "      <th>一大堆</th>\n",
       "      <th>一大批</th>\n",
       "      <th>一套</th>\n",
       "      <th>一委</th>\n",
       "      <th>...</th>\n",
       "      <th>鞋子</th>\n",
       "      <th>顺丰</th>\n",
       "      <th>顺遂</th>\n",
       "      <th>领导</th>\n",
       "      <th>验证</th>\n",
       "      <th>验车</th>\n",
       "      <th>骑手</th>\n",
       "      <th>高估</th>\n",
       "      <th>鸡蛋</th>\n",
       "      <th>黑市</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   一个月  一位  一刻  一周  一圈  一夜之间  一大堆  一大批  一套  一委  ...  鞋子  顺丰  顺遂  领导  验证  验车  \\\n",
       "0    0   0   1   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "1    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   1   0   0   \n",
       "2    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "3    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "4    1   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "   骑手  高估  鸡蛋  黑市  \n",
       "0   0   0   0   0  \n",
       "1   0   0   0   0  \n",
       "2   0   0   0   0  \n",
       "3   0   0   0   0  \n",
       "4   0   0   0   0  \n",
       "\n",
       "[5 rows x 696 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5b10b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=frozenset(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61ca760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc80f221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一个月</th>\n",
       "      <th>一位</th>\n",
       "      <th>一刻</th>\n",
       "      <th>一周</th>\n",
       "      <th>一圈</th>\n",
       "      <th>一夜之间</th>\n",
       "      <th>一大堆</th>\n",
       "      <th>一大批</th>\n",
       "      <th>一套</th>\n",
       "      <th>一委</th>\n",
       "      <th>...</th>\n",
       "      <th>鞋子</th>\n",
       "      <th>顺丰</th>\n",
       "      <th>顺遂</th>\n",
       "      <th>领导</th>\n",
       "      <th>验证</th>\n",
       "      <th>验车</th>\n",
       "      <th>骑手</th>\n",
       "      <th>高估</th>\n",
       "      <th>鸡蛋</th>\n",
       "      <th>黑市</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   一个月  一位  一刻  一周  一圈  一夜之间  一大堆  一大批  一套  一委  ...  鞋子  顺丰  顺遂  领导  验证  验车  \\\n",
       "0    0   0   1   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "1    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   1   0   0   \n",
       "2    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "3    0   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "4    1   0   0   0   0     0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "   骑手  高估  鸡蛋  黑市  \n",
       "0   0   0   0   0  \n",
       "1   0   0   0   0  \n",
       "2   0   0   0   0  \n",
       "3   0   0   0   0  \n",
       "4   0   0   0   0  \n",
       "\n",
       "[5 rows x 696 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eae1c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一刻</th>\n",
       "      <th>三顿</th>\n",
       "      <th>上班</th>\n",
       "      <th>仓鼠</th>\n",
       "      <th>代购</th>\n",
       "      <th>健健康康</th>\n",
       "      <th>健康</th>\n",
       "      <th>办公</th>\n",
       "      <th>可惜</th>\n",
       "      <th>同事</th>\n",
       "      <th>...</th>\n",
       "      <th>结束</th>\n",
       "      <th>老板</th>\n",
       "      <th>能力</th>\n",
       "      <th>花草</th>\n",
       "      <th>解封</th>\n",
       "      <th>这是</th>\n",
       "      <th>邻居</th>\n",
       "      <th>防疫</th>\n",
       "      <th>隔离</th>\n",
       "      <th>领导</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   一刻  三顿  上班  仓鼠  代购  健健康康  健康  办公  可惜  同事  ...  结束  老板  能力  花草  解封  这是  邻居  \\\n",
       "0   1   0   0   0   0     0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0     0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0     0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0     0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "4   0   0   0   0   0     0   0   0   0   0  ...   0   1   0   0   0   0   0   \n",
       "\n",
       "   防疫  隔离  领导  \n",
       "0   0   1   0  \n",
       "1   0   0   1  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df, \n",
    "                       min_df = min_df, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(tt['del']).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4fab3",
   "metadata": {},
   "source": [
    "## 词频统计 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77b4cc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮 开启 滞留 人员 陆续 返乡 安徽 江苏 河南 返乡 人员 ',\n",
       " '疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 一条 汉奸 真的 太 抗战 那会 太 敏感 我乎 审核 说 一条 有人 发国难财 能量 不小 ',\n",
       " '疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼一闭 当作 事情 发生 岁月 静好 叫唤 ',\n",
       " '民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区 ',\n",
       " '明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 舟   中国新闻社 ',\n",
       " '疫情 民生 物资   黑市     ',\n",
       " '民生 疫情   外卖 骑手 好自为之 骑手 代购 袋 泡面  鸡蛋 收取  元 警方 调查 惊人 发现 最新消息 证实 赵某 因涉嫌 非法经营 罪 长宁 警方 依法 强制措施 案件 进一步 侦办 中 ',\n",
       " '民生 疫情   连花清 瘟 牛 逼 连花清 瘟 站台  收钱 办事 钱能 挣 一大批 粉丝 数十万 数百万  金  纷纷 连花清 瘟 站台 证实  收钱 连花清 瘟 站台 事实   ',\n",
       " '疫情 民生   救护车 谎称 进 小区 救人 买  条烟 业主 震怒 报 次警 近日 救护车 谎称 进 小区 救人 实则 买烟 新闻 引发 关注 ',\n",
       " '疫情 民生   跑腿 代购 牛 逼   ',\n",
       " '疫情 民生 刚刚 明白   网传 真的 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚 ',\n",
       " '疫情   核酸 检测 强烈建议 专业 事要 专业 人来 做 楼 里  未 拉走 居委会 老先生 老太太 志愿者 包括 毛估估 漏洞百出   ',\n",
       " '疫情 民生 有钱人 玩 事关 老百姓 民生 呼吁 政府部门 管管   ',\n",
       " '疫情 民生   不算 太贵 老板 还算 良心 关键 至少 送货 只会 送 烂 菜 枯叶 要强   ',\n",
       " '疫情 民生 抗疫 物资   这是 真的 话 抓 枪毙     ',\n",
       " '疫情 民生   民间 自发 团购 能力 配货 送货 政府 组织 商委   市场 管理局   ',\n",
       " '疫情   基层干部 太 当回事 能力 强 应付 上级 忽悠 下级 本事 一大堆 资料 一套 一套 关键 不行 小区 志愿者 义工 值得反思   ',\n",
       " '疫情   不要脸 东西 胡说八道   ',\n",
       " '疫情 基层 一线 干部 累 趴 确实 好多 居民 需求 人理 月 号 核酸 阳性 一轮 结束 拉走 盼望 政府  转 疾控   普陀 桃浦   宣克炅   江 丄 孤 舟   ',\n",
       " '咋啦 核酸 阳性反应 居家 隔离 普陀 桃浦 疾控   ',\n",
       " '这是 在建 方舱 施工 宣克炅   嘉定 公安   嘉定 安监   华盛 国际 商务 花园 北区 ',\n",
       " '疫情 大白菜  块钱 一颗 青菜  块钱 一斤 高估 承压 能力 月  日 国务院 联防 联控 机制 督查组 沪 防疫 方式 加速 转轨 疫情 世外桃源 居民 生计 疫情 导致 经济 压力 打击 ',\n",
       " '前两天 想 说 江桥 菜市场 封控 运菜 货车 周边 停留 自律   阳光 威尼斯 提香 湾 ',\n",
       " '防疫   不到 小时 连续 两份 通知 防控 基层 一线 混乱 镇 疫情 防控 指挥部 指示 阳光 一委  区 封控  区 解封 时间 待镇 疫情 防控 指挥部 指示 一刻 咒术 回战 冒个 泡 ',\n",
       " '百 果园 趁火打劫 百 果园 ',\n",
       " '党校 教授 求教 再驳 党校 教授 降美弃 俄     文 司马南     俄罗斯 倒下 中国 西方 围剿 司马南 百度 动态 发表 新 动态 ',\n",
       " '六院 疫情 舆论 焦点 网上 该院 发生 院内 感染 院方 负有 隐瞒 消息 处置 胡锡进 百度 动态 发表 新 动态 ',\n",
       " ' 星期天 妈妈 办公 ',\n",
       " '一周 三顿  感谢 邻居 投 冰淇淋  ',\n",
       " '居家 一个月 焦 大哥 皮肤 不行 小可爱 做 核酸 乖到 不行 ',\n",
       " '太爱 吃 三明治  烤鸡 腿  蔬菜 饼  疫情 三顿 厨娘 可惜 半个 月 好多 拍照 好好 记录 一顿 ',\n",
       " '一刻   周五  我来 干扰 妈妈 居家 办公 ',\n",
       " '真会 挑 下雨天   睡懒觉 感觉 孩子 中 长大 ',\n",
       " '体重  想 时隔 十几年 是因为 疫情 生个 孩子 瘦 内心 开心 回奶 瘦 慢慢 吃 断奶 瘦 ',\n",
       " '妈妈 发来 航拍 照片 爸爸 弄 泉水 源 太美 太想 回去 期待 国庆 回老家 ',\n",
       " '卷毛  陪 妈妈 居家 办公  可惜 天气    只能 窗台 玩 ',\n",
       " '何德何能 楼里 医院 邻居  有盒 马 员工 邻居 幸福 好好 生活 好好 吃饭 健健康康 防护  疫情 早点 结束   ',\n",
       " '健康 平安  疫情 早日 结束 ',\n",
       " '浦西 自由 明天 足不出户 保佑 平安 顺遂 健康 疫情 无情 邻里 有情 感恩   愿 孩子 爸爸 永远 健康 快乐 爱    ',\n",
       " '家门 美景     ',\n",
       " '上班  节后 健健康康 平平安安 加油      ',\n",
       " '三个 月 变化 孩子 肉眼 一圈 身高  肉眼 瘦 一圈 ',\n",
       " '沉浸 式 开箱 开箱   月  法棍 终于 犒劳 一年 真的 涨价 前入 幸福 买到 赚     ',\n",
       " '聂总说 国内 喝咖啡 奶茶 最终 喝牛奶  哈哈哈哈 解封 回归 工作 聂总投     ',\n",
       " '油 加满   一杯 抹 茶   出发 上班   上班 幸福 事情       ',\n",
       " '日上 真的 太给力 凌晨 下单 下午 不敢相信 这是 疫情 顺丰 快递 实名  日上 弥补 两年 国门 囤货 遗憾 压根 不用 囤 快完了 买 行 代购 两年 省  娇韵诗   日上 免税店 ',\n",
       " '决赛圈 出门 希望 健健康康 ',\n",
       " '老板 同事 太好了 心疼 家里 孩子 居家 办公 感恩   加油  希望 决赛圈 健健康康 疫情 居家 办公 ',\n",
       " '疫情 不易 楼下 新开  门店 支持  喝 咖啡 来个 抹 茶 铁       长阳 创谷 ',\n",
       " '一周 喝 两杯  部门 同事 离职 请 喝茶 饮 开心 朋友 天蝎座   辣妈 ',\n",
       " '开心 美丽 三月 十年 前买 第一辆 车 时间 验车  终于 一辆        金桥 ',\n",
       " '张老师   爱   女神 节 快乐   傩 晃晃 ',\n",
       " '老板 变 去年 请 喝 奶茶 喝 两杯 过分 ',\n",
       " '快乐 可爱 善良 邻居 疫情   ',\n",
       " '苹果 花园 团购    救救 孩子 封控 多天 求个 谱 苹果 花园 团购 联系方式 做 团长 坐标 青浦   ',\n",
       " '京东 快递   说 快递 解封 前 收到 希望 京东 早日 解控 早日 派送   ',\n",
       " '物资   物资 发了 感谢 内心  物资 小区 实力   ',\n",
       " '太 适合   ',\n",
       " '周四 快乐   ',\n",
       " '隔离 在家 第四天 三顿 饭 缩减 两顿 不解 封 只能 一顿 抗击 疫情 疫情   ',\n",
       " '我司 男生 想 变 女生     谢谢 宠爱   ',\n",
       " '公司  节 礼物  拿捏   ',\n",
       " '周一 惊喜 感谢 领导 女神 节 仪式 感   ',\n",
       " '徐汇 今晚 载入史册 ',\n",
       " '腾讯 成长 守护 平台 恶意 绑定 未成年人 账户 ',\n",
       " '疫情   正 能量 天使 守护星  ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫                  ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   哇塞 小区 收到  玖 少年 团肖战    工作室   影音 官微   捐赠 礼包 忙碌 亮点 感谢   ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   砖家 抗疫 ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫 ',\n",
       " '疫情 妞妞 端午 花草 家有 糖 猫   硬 隔离 火宅 新冠 死法 都行 ',\n",
       " '疫情   演员 成本 高 网友 扒 新闻   ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   众人 拾柴 ',\n",
       " '疫情 仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫    屯菜    ',\n",
       " '仓鼠 妞妞 端午 水彩 押花 花草 家有 糖 猫    ',\n",
       " '仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫   夜奔    ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   流传 一段  多分钟 录音 一位 男性 市民 疾控中心 女 领导 对话   这位 男士 父母 酒店 隔离 前天 做 核酸 健康 云上 阴性 接到 疾控中心 电话 说 阳性 做   专家 这位 女 领导 太极 说出 心声   疾控 健康 云 医疗机构 各自为政 混乱   家里 老人 方舱 医院 居家 隔离 观察 无症状 休息 三五天    拉 老人 强制 隔离 出示 阳性 证明 概率   有用 知识 无症状 轻症 居家 隔离 更优 选择 病 特别 治疗 对付 感冒 扛过去   弄 方舱 医院 条件 很差 治疗 不利于 病人 康复     专业 人员 发现 轻症 无症 转走 家里 隔离 提  次 有人 听过 没人 听   逼 疯 专业 机构 逼 疯 专业 人员 说 没人 听 病 政治性 疾病   录音 传播 直白 话语 说出 共识 有人 评论 里 说 领导 危险 说 真话 做 人事 希望 女 领导 越来越 ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫   仓鼠 喝奶    ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫    ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫 ',\n",
       " '疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫    ',\n",
       " '妞妞 端午 花草 水彩 押花   小知 女娃娃    ',\n",
       " '妞妞 端午 花草 水彩 押花 家有 糖 猫   吾 家 女 初长成 ',\n",
       " '真他妈 烦透了   真的 垃圾 ',\n",
       " '疫情   物价 离谱 ',\n",
       " '春天 完 哭 失眠 久久 平复 ',\n",
       " '早  晚 后续 脸 过敏   眼下 脱皮 过敏 地方 一夜之间 长出 皱纹 真的 会谢   无良 商家 营销 号  ',\n",
       " '天天 辟谣 谣言 验证 事实 ',\n",
       " '真特 脑残 ',\n",
       " '一位 同事 喜提  天 组 疫情 ',\n",
       " '新增  例 本土 确诊  例 无症状 日增 两千多 影响 上班 ',\n",
       " '新增   影响 上班 地铁 依旧 人挤 工人 可太 卑微 ',\n",
       " '晚上  登陆 微信 吓人 ',\n",
       " '防疫 措施 无语 ',\n",
       " '社会 毒打   终于 悟出 道理 找对象   帅不帅 一点   搞钱   有钱 第一   丑 丑   帅不帅   看多 人烦 ',\n",
       " '想 辞职 第一天 ',\n",
       " '新 鞋子 哐 哐响 新 裙子 大步 走路   这才 周一   犯傻 ',\n",
       " '人世间 细节 处 锅包肉 ',\n",
       " 'の 天气 阴晴不定 有时候 像是 漫画 里 走 の 天气 有时候 狂风骤雨 爱 の 季节 太 可惜 晚安   愿 解封 浪 先睡 敬 晚安    明天 卖菜 の ',\n",
       " '一刻 宝宝 少女 心 天气 天意   啥时候 个头 赶紧 倒闭 叭叭 叭 求求 ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48b7bc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'疫情 民生 一刻 返乡   返乡 潮 开启 安徽 江苏 河南人 隔离 做 核酸 钱 返乡 潮 开启 滞留 人员 陆续 返乡 安徽 江苏 河南 返乡 人员  疫情 民生 现状   现状 忍不住 瞎 想 外行 领导 内行 内行 闭嘴 同理 衍生 行业 一条 汉奸 真的 太 抗战 那会 太 敏感 我乎 审核 说 一条 有人 发国难财 能量 不小  疫情 民生             无间道 罗生门 本来 人生 如戏 是我太 算了 干脆 眼一闭 当作 事情 发生 岁月 静好 叫唤  民生 疫情 团长   牛 逼 团长 开个 车照 送 封控 小区  明康汇 采购员 成功 老板 一个月 光辉 形象 給黑 明康汇   市场监管   江 丄 孤 舟   中国新闻社  疫情 民生 物资   黑市      民生 疫情   外卖 骑手 好自为之 骑手 代购 袋 泡面  鸡蛋 收取  元 警方 调查 惊人 发现 最新消息 证实 赵某 因涉嫌 非法经营 罪 长宁 警方 依法 强制措施 案件 进一步 侦办 中  民生 疫情   连花清 瘟 牛 逼 连花清 瘟 站台  收钱 办事 钱能 挣 一大批 粉丝 数十万 数百万  金  纷纷 连花清 瘟 站台 证实  收钱 连花清 瘟 站台 事实    疫情 民生   救护车 谎称 进 小区 救人 买  条烟 业主 震怒 报 次警 近日 救护车 谎称 进 小区 救人 实则 买烟 新闻 引发 关注  疫情 民生   跑腿 代购 牛 逼    疫情 民生 刚刚 明白   网传 真的 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚 网传 嘉定 静默 天 取消 快递 嘉定区 防控 办 月   日   市 统一 部署   全区 综合 防控 措施   发起 清零 攻坚  疫情   核酸 检测 强烈建议 专业 事要 专业 人来 做 楼 里  未 拉走 居委会 老先生 老太太 志愿者 包括 毛估估 漏洞百出    疫情 民生 有钱人 玩 事关 老百姓 民生 呼吁 政府部门 管管    疫情 民生   不算 太贵 老板 还算 良心 关键 至少 送货 只会 送 烂 菜 枯叶 要强    疫情 民生 抗疫 物资   这是 真的 话 抓 枪毙      疫情 民生   民间 自发 团购 能力 配货 送货 政府 组织 商委   市场 管理局    疫情   基层干部 太 当回事 能力 强 应付 上级 忽悠 下级 本事 一大堆 资料 一套 一套 关键 不行 小区 志愿者 义工 值得反思    疫情   不要脸 东西 胡说八道    疫情 基层 一线 干部 累 趴 确实 好多 居民 需求 人理 月 号 核酸 阳性 一轮 结束 拉走 盼望 政府  转 疾控   普陀 桃浦   宣克炅   江 丄 孤 舟    咋啦 核酸 阳性反应 居家 隔离 普陀 桃浦 疾控    这是 在建 方舱 施工 宣克炅   嘉定 公安   嘉定 安监   华盛 国际 商务 花园 北区  疫情 大白菜  块钱 一颗 青菜  块钱 一斤 高估 承压 能力 月  日 国务院 联防 联控 机制 督查组 沪 防疫 方式 加速 转轨 疫情 世外桃源 居民 生计 疫情 导致 经济 压力 打击  前两天 想 说 江桥 菜市场 封控 运菜 货车 周边 停留 自律   阳光 威尼斯 提香 湾  防疫   不到 小时 连续 两份 通知 防控 基层 一线 混乱 镇 疫情 防控 指挥部 指示 阳光 一委  区 封控  区 解封 时间 待镇 疫情 防控 指挥部 指示 一刻 咒术 回战 冒个 泡  百 果园 趁火打劫 百 果园  党校 教授 求教 再驳 党校 教授 降美弃 俄     文 司马南     俄罗斯 倒下 中国 西方 围剿 司马南 百度 动态 发表 新 动态  六院 疫情 舆论 焦点 网上 该院 发生 院内 感染 院方 负有 隐瞒 消息 处置 胡锡进 百度 动态 发表 新 动态   星期天 妈妈 办公  一周 三顿  感谢 邻居 投 冰淇淋   居家 一个月 焦 大哥 皮肤 不行 小可爱 做 核酸 乖到 不行  太爱 吃 三明治  烤鸡 腿  蔬菜 饼  疫情 三顿 厨娘 可惜 半个 月 好多 拍照 好好 记录 一顿  一刻   周五  我来 干扰 妈妈 居家 办公  真会 挑 下雨天   睡懒觉 感觉 孩子 中 长大  体重  想 时隔 十几年 是因为 疫情 生个 孩子 瘦 内心 开心 回奶 瘦 慢慢 吃 断奶 瘦  妈妈 发来 航拍 照片 爸爸 弄 泉水 源 太美 太想 回去 期待 国庆 回老家  卷毛  陪 妈妈 居家 办公  可惜 天气    只能 窗台 玩  何德何能 楼里 医院 邻居  有盒 马 员工 邻居 幸福 好好 生活 好好 吃饭 健健康康 防护  疫情 早点 结束    健康 平安  疫情 早日 结束  浦西 自由 明天 足不出户 保佑 平安 顺遂 健康 疫情 无情 邻里 有情 感恩   愿 孩子 爸爸 永远 健康 快乐 爱     家门 美景      上班  节后 健健康康 平平安安 加油       三个 月 变化 孩子 肉眼 一圈 身高  肉眼 瘦 一圈  沉浸 式 开箱 开箱   月  法棍 终于 犒劳 一年 真的 涨价 前入 幸福 买到 赚      聂总说 国内 喝咖啡 奶茶 最终 喝牛奶  哈哈哈哈 解封 回归 工作 聂总投      油 加满   一杯 抹 茶   出发 上班   上班 幸福 事情        日上 真的 太给力 凌晨 下单 下午 不敢相信 这是 疫情 顺丰 快递 实名  日上 弥补 两年 国门 囤货 遗憾 压根 不用 囤 快完了 买 行 代购 两年 省  娇韵诗   日上 免税店  决赛圈 出门 希望 健健康康  老板 同事 太好了 心疼 家里 孩子 居家 办公 感恩   加油  希望 决赛圈 健健康康 疫情 居家 办公  疫情 不易 楼下 新开  门店 支持  喝 咖啡 来个 抹 茶 铁       长阳 创谷  一周 喝 两杯  部门 同事 离职 请 喝茶 饮 开心 朋友 天蝎座   辣妈  开心 美丽 三月 十年 前买 第一辆 车 时间 验车  终于 一辆        金桥  张老师   爱   女神 节 快乐   傩 晃晃  老板 变 去年 请 喝 奶茶 喝 两杯 过分  快乐 可爱 善良 邻居 疫情    苹果 花园 团购    救救 孩子 封控 多天 求个 谱 苹果 花园 团购 联系方式 做 团长 坐标 青浦    京东 快递   说 快递 解封 前 收到 希望 京东 早日 解控 早日 派送    物资   物资 发了 感谢 内心  物资 小区 实力    太 适合    周四 快乐    隔离 在家 第四天 三顿 饭 缩减 两顿 不解 封 只能 一顿 抗击 疫情 疫情    我司 男生 想 变 女生     谢谢 宠爱    公司  节 礼物  拿捏    周一 惊喜 感谢 领导 女神 节 仪式 感    徐汇 今晚 载入史册  腾讯 成长 守护 平台 恶意 绑定 未成年人 账户  疫情   正 能量 天使 守护星   疫情 妞妞 端午 花草 家有 糖 猫                   疫情 妞妞 端午 花草 家有 糖 猫   哇塞 小区 收到  玖 少年 团肖战    工作室   影音 官微   捐赠 礼包 忙碌 亮点 感谢    疫情 妞妞 端午 花草 家有 糖 猫   砖家 抗疫  妞妞 端午 花草 水彩 押花 家有 糖 猫  疫情 妞妞 端午 花草 家有 糖 猫   硬 隔离 火宅 新冠 死法 都行  疫情   演员 成本 高 网友 扒 新闻    疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   众人 拾柴  疫情 仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫    屯菜     仓鼠 妞妞 端午 水彩 押花 花草 家有 糖 猫     仓鼠 妞妞 端午 花草 水彩 押花 家有 糖 猫   夜奔     疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫   流传 一段  多分钟 录音 一位 男性 市民 疾控中心 女 领导 对话   这位 男士 父母 酒店 隔离 前天 做 核酸 健康 云上 阴性 接到 疾控中心 电话 说 阳性 做   专家 这位 女 领导 太极 说出 心声   疾控 健康 云 医疗机构 各自为政 混乱   家里 老人 方舱 医院 居家 隔离 观察 无症状 休息 三五天    拉 老人 强制 隔离 出示 阳性 证明 概率   有用 知识 无症状 轻症 居家 隔离 更优 选择 病 特别 治疗 对付 感冒 扛过去   弄 方舱 医院 条件 很差 治疗 不利于 病人 康复     专业 人员 发现 轻症 无症 转走 家里 隔离 提  次 有人 听过 没人 听   逼 疯 专业 机构 逼 疯 专业 人员 说 没人 听 病 政治性 疾病   录音 传播 直白 话语 说出 共识 有人 评论 里 说 领导 危险 说 真话 做 人事 希望 女 领导 越来越  妞妞 端午 花草 水彩 押花 家有 糖 猫   仓鼠 喝奶     妞妞 端午 花草 水彩 押花 家有 糖 猫     疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫  疫情 妞妞 端午 花草 水彩 押花 家有 糖 猫     妞妞 端午 花草 水彩 押花   小知 女娃娃     妞妞 端午 花草 水彩 押花 家有 糖 猫   吾 家 女 初长成  真他妈 烦透了   真的 垃圾  疫情   物价 离谱  春天 完 哭 失眠 久久 平复  早  晚 后续 脸 过敏   眼下 脱皮 过敏 地方 一夜之间 长出 皱纹 真的 会谢   无良 商家 营销 号   天天 辟谣 谣言 验证 事实  真特 脑残  一位 同事 喜提  天 组 疫情  新增  例 本土 确诊  例 无症状 日增 两千多 影响 上班  新增   影响 上班 地铁 依旧 人挤 工人 可太 卑微  晚上  登陆 微信 吓人  防疫 措施 无语  社会 毒打   终于 悟出 道理 找对象   帅不帅 一点   搞钱   有钱 第一   丑 丑   帅不帅   看多 人烦  想 辞职 第一天  新 鞋子 哐 哐响 新 裙子 大步 走路   这才 周一   犯傻  人世间 细节 处 锅包肉  の 天气 阴晴不定 有时候 像是 漫画 里 走 の 天气 有时候 狂风骤雨 爱 の 季节 太 可惜 晚安   愿 解封 浪 先睡 敬 晚安    明天 卖菜 の  一刻 宝宝 少女 心 天气 天意   啥时候 个头 赶紧 倒闭 叭叭 叭 求求 '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = ' '.join(u_emoji)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8308c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(words):\n",
    "    count = []\n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(words)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "            s = words[i]\n",
    "            if(s in affect_dict[idx]):\n",
    "                affect_cnt += 1\n",
    "        \n",
    "        #计算比率\n",
    "        r_affect = 0.0\n",
    "        if (cnt_tags > 0):\n",
    "            r_affect = affect_cnt/cnt_tags\n",
    "        \n",
    "        new = str(affect_col_list[idx])+' '+str(affect_cnt)+' '+str(r_affect)\n",
    "        count.append(new) \n",
    "        idx += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bad2ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = []\n",
    "for line in u_emoji:\n",
    "    line_u = str(line)\n",
    "    line_uu = line_u.split()\n",
    "    aaa.append(line_uu)\n",
    "\n",
    "new=sum(aaa,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d341656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_cun = feature_extraction(new) # 这里的返回值是字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd0e2a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PA 32 0.023827252419955324',\n",
       " 'PE 3 0.0022338049143708115',\n",
       " 'PD 3 0.0022338049143708115',\n",
       " 'PH 19 0.014147431124348473',\n",
       " 'PG 2 0.0014892032762472078',\n",
       " 'PB 4 0.0029784065524944155',\n",
       " 'PK 6 0.004467609828741623',\n",
       " 'NA 1 0.0007446016381236039',\n",
       " 'NB 6 0.004467609828741623',\n",
       " 'NJ 5 0.0037230081906180195',\n",
       " 'NH 0 0.0',\n",
       " 'PF 2 0.0014892032762472078',\n",
       " 'NI 0 0.0',\n",
       " 'NC 1 0.0007446016381236039',\n",
       " 'NG 1 0.0007446016381236039',\n",
       " 'NE 3 0.0022338049143708115',\n",
       " 'ND 2 0.0014892032762472078',\n",
       " 'NN 28 0.02084884586746091',\n",
       " 'NK 0 0.0',\n",
       " 'NL 0 0.0',\n",
       " 'PC 2 0.0014892032762472078',\n",
       " 'MH 20 0.014892032762472078',\n",
       " 'MS 4 0.0029784065524944155',\n",
       " 'MA 0 0.0',\n",
       " 'MD 2 0.0014892032762472078',\n",
       " 'ME 4 0.0029784065524944155',\n",
       " 'P 0 0.0',\n",
       " 'N 0 0.0',\n",
       " 'Ne 0 0.0']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affect_cun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f17b18",
   "metadata": {},
   "source": [
    "## 主题建模尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    " \n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    " \n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a9600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf974903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cut_content</th>\n",
       "      <th>del</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#上海民生##上海疫情##核酸#检测《上海人核酸“续命”尴尬：缺管子、乱加码、排队最长3小时...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>2022-06-02 13:22:28</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 核酸 # 检测 《 上海 人 核酸 “ 续命 ” 尴...</td>\n",
       "      <td>民生 疫情 核酸 检测 核酸 续命 尴尬 缺 管子 乱 加码 排队 最长 小时 核酸 续命 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#上海民生##上海疫情##上海复工#上海醒来，6月1号不会是哄孩子的吧。6月1日零时起 上海...</td>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>2022-05-30 19:42:39</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 ## 上海 复工 # 上海 醒来 ， 6 月 1 号 不会...</td>\n",
       "      <td>民生 疫情 复工 醒来 月 号 哄 孩子 月 日 零时   有序 恢复 住宅小区 出入 公共...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#上海民生##上海疫情# 有关团长《独家｜起底“团长江湖”：平价被抵制、供应商“强制”加价、...</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>2022-05-16 09:38:37</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 #   有关 团长 《 独家 ｜ 起底 “ 团长 江湖 ”...</td>\n",
       "      <td>民生 疫情   团长 独家 起底 团长 江湖 平价 抵制 供应商 强制 加价 黑白 团长 大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#居委会##上海民生##上海疫情# 上海奇葩居委干部，假借抗疫强索私吞翡翠，官方属实》一个居...</td>\n",
       "      <td>2022-05-07</td>\n",
       "      <td>2022-05-07 23:39:52</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 居委会 ## 上海 民生 ## 上海 疫情 #   上海 奇葩 居委 干部 ， 假借 抗...</td>\n",
       "      <td>居委会 民生 疫情   奇葩 居委 干部 假借 抗疫 强索 私吞 翡翠 官方 属实 居委 干...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#上海民生##上海疫情# 祝贺🎉五一劳动节，迎接双满月，你劳动了吗？</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022-05-01 17:34:27</td>\n",
       "      <td>1000074972</td>\n",
       "      <td># 上海 民生 ## 上海 疫情 #   祝贺 🎉 五一劳动节 ， 迎接 双 满月 ， 你 ...</td>\n",
       "      <td>民生 疫情   祝贺  五一劳动节 迎接 双 满月 劳动</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  #上海民生##上海疫情##核酸#检测《上海人核酸“续命”尴尬：缺管子、乱加码、排队最长3小时...  2022-06-02   \n",
       "1  #上海民生##上海疫情##上海复工#上海醒来，6月1号不会是哄孩子的吧。6月1日零时起 上海...  2022-05-30   \n",
       "2  #上海民生##上海疫情# 有关团长《独家｜起底“团长江湖”：平价被抵制、供应商“强制”加价、...  2022-05-16   \n",
       "3  #居委会##上海民生##上海疫情# 上海奇葩居委干部，假借抗疫强索私吞翡翠，官方属实》一个居...  2022-05-07   \n",
       "4                #上海民生##上海疫情# 祝贺🎉五一劳动节，迎接双满月，你劳动了吗？   2022-05-01   \n",
       "\n",
       "                  time     user_id  \\\n",
       "0  2022-06-02 13:22:28  1000074972   \n",
       "1  2022-05-30 19:42:39  1000074972   \n",
       "2  2022-05-16 09:38:37  1000074972   \n",
       "3  2022-05-07 23:39:52  1000074972   \n",
       "4  2022-05-01 17:34:27  1000074972   \n",
       "\n",
       "                                         cut_content  \\\n",
       "0  # 上海 民生 ## 上海 疫情 ## 核酸 # 检测 《 上海 人 核酸 “ 续命 ” 尴...   \n",
       "1  # 上海 民生 ## 上海 疫情 ## 上海 复工 # 上海 醒来 ， 6 月 1 号 不会...   \n",
       "2  # 上海 民生 ## 上海 疫情 #   有关 团长 《 独家 ｜ 起底 “ 团长 江湖 ”...   \n",
       "3  # 居委会 ## 上海 民生 ## 上海 疫情 #   上海 奇葩 居委 干部 ， 假借 抗...   \n",
       "4  # 上海 民生 ## 上海 疫情 #   祝贺 🎉 五一劳动节 ， 迎接 双 满月 ， 你 ...   \n",
       "\n",
       "                                                 del  \n",
       "0  民生 疫情 核酸 检测 核酸 续命 尴尬 缺 管子 乱 加码 排队 最长 小时 核酸 续命 ...  \n",
       "1  民生 疫情 复工 醒来 月 号 哄 孩子 月 日 零时   有序 恢复 住宅小区 出入 公共...  \n",
       "2  民生 疫情   团长 独家 起底 团长 江湖 平价 抵制 供应商 强制 加价 黑白 团长 大...  \n",
       "3  居委会 民生 疫情   奇葩 居委 干部 假借 抗疫 强索 私吞 翡翠 官方 属实 居委 干...  \n",
       "4                      民生 疫情   祝贺  五一劳动节 迎接 双 满月 劳动   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读入数据\n",
    "group_2_del = pd.read_csv('D:\\my research\\group\\group_2\\group_2_final.csv',encoding='utf-8')\n",
    "group_2_del.columns = ['0','content','date','time','user_id','cut_content','del']\n",
    "group_2_del = group_2_del.drop(columns='0')\n",
    "group_2_del.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4296f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造 TF-IDF\n",
    "#tf_idf_vectorizer = TfidfVectorizer()\n",
    "#tf_idf = tf_idf_vectorizer.fit_transform(tt['del'])\n",
    "tf_idf_vectorizer = TfidfVectorizer(min_df = 0.001,max_df = 0.8)\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(group_2_del['del'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de00e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        nan  一下子  一个多月  一个月   一人   一件   一份   一会  一会儿   一位  ...   高考   魅力   魔幻  \\\n",
      "0       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...     ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "174203  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "174204  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "174205  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "174206  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "174207  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "         鱼币   鸡蛋   麻烦   黄瓜   黑暗   鼓励   龚俊  \n",
      "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  \n",
      "174203  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "174204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "174205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "174206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "174207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[174208 rows x 1480 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nan</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一个多月</th>\n",
       "      <th>一个月</th>\n",
       "      <th>一人</th>\n",
       "      <th>一件</th>\n",
       "      <th>一份</th>\n",
       "      <th>一会</th>\n",
       "      <th>一会儿</th>\n",
       "      <th>一位</th>\n",
       "      <th>...</th>\n",
       "      <th>高考</th>\n",
       "      <th>魅力</th>\n",
       "      <th>魔幻</th>\n",
       "      <th>鱼币</th>\n",
       "      <th>鸡蛋</th>\n",
       "      <th>麻烦</th>\n",
       "      <th>黄瓜</th>\n",
       "      <th>黑暗</th>\n",
       "      <th>鼓励</th>\n",
       "      <th>龚俊</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174203</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174206</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174207</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174208 rows × 1480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        nan  一下子  一个多月  一个月   一人   一件   一份   一会  一会儿   一位  ...   高考   魅力   魔幻  \\\n",
       "0       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4       0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...     ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "174203  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "174204  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "174205  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "174206  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "174207  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "         鱼币   鸡蛋   麻烦   黄瓜   黑暗   鼓励   龚俊  \n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  \n",
       "174203  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "174204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "174205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "174206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "174207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[174208 rows x 1480 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征词列表\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "# 特征词 TF-IDF 矩阵\n",
    "matrix = tf_idf.toarray()\n",
    "feature_names_df = pd.DataFrame(matrix,columns=feature_names)\n",
    "print(feature_names_df)\n",
    "feature_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21c737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=50, n_components=5, random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 lda 主题数\n",
    "n_topics = 5\n",
    "# 要输出的每个主题的前 n_top_words 个主题词数\n",
    "n_top_words = 20\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0)\n",
    "# 核心，给 LDA 喂生成的 TF-IDF 矩阵\n",
    "lda.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036b0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_data_frame(model: LatentDirichletAllocation,\n",
    "                         tf_idf_vectorizer: TfidfVectorizer,\n",
    "                         n_top_words: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出每个主题的前 n_top_words 个词\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    tf_idf_vectorizer : sklearn 的 TfidfVectorizer\n",
    "    n_top_words :前 n_top_words 个主题词\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    rows = []\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        rows.append(top_words)\n",
    "    columns = [f'topic {i+1}' for i in range(n_top_words)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_to_data_frame(model: LatentDirichletAllocation, X: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出文档主题概率分布情况\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    X : 词向量矩阵\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    # 求出给定文档的主题概率分布矩阵\n",
    "    matrix = model.transform(X)\n",
    "    columns = [f'P(topic {i+1})' for i in range(len(model.components_))]\n",
    "    df = pd.DataFrame(matrix, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3747974a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P(topic 1)</th>\n",
       "      <th>P(topic 2)</th>\n",
       "      <th>P(topic 3)</th>\n",
       "      <th>P(topic 4)</th>\n",
       "      <th>P(topic 5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051159</td>\n",
       "      <td>0.051350</td>\n",
       "      <td>0.795131</td>\n",
       "      <td>0.051134</td>\n",
       "      <td>0.051226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036899</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.852769</td>\n",
       "      <td>0.036805</td>\n",
       "      <td>0.036825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039718</td>\n",
       "      <td>0.039852</td>\n",
       "      <td>0.039754</td>\n",
       "      <td>0.840954</td>\n",
       "      <td>0.039722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.059192</td>\n",
       "      <td>0.059356</td>\n",
       "      <td>0.059370</td>\n",
       "      <td>0.762916</td>\n",
       "      <td>0.059166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049717</td>\n",
       "      <td>0.800956</td>\n",
       "      <td>0.049696</td>\n",
       "      <td>0.049777</td>\n",
       "      <td>0.049855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.668273</td>\n",
       "      <td>0.082917</td>\n",
       "      <td>0.082924</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.082935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.052380</td>\n",
       "      <td>0.052354</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>0.790546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.073314</td>\n",
       "      <td>0.073301</td>\n",
       "      <td>0.073293</td>\n",
       "      <td>0.706780</td>\n",
       "      <td>0.073313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.817572</td>\n",
       "      <td>0.045743</td>\n",
       "      <td>0.045540</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.045603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.813676</td>\n",
       "      <td>0.046672</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.046530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    P(topic 1)  P(topic 2)  P(topic 3)  P(topic 4)  P(topic 5)\n",
       "0     0.051159    0.051350    0.795131    0.051134    0.051226\n",
       "1     0.036899    0.036703    0.852769    0.036805    0.036825\n",
       "2     0.039718    0.039852    0.039754    0.840954    0.039722\n",
       "3     0.059192    0.059356    0.059370    0.762916    0.059166\n",
       "4     0.049717    0.800956    0.049696    0.049777    0.049855\n",
       "..         ...         ...         ...         ...         ...\n",
       "95    0.668273    0.082917    0.082924    0.082951    0.082935\n",
       "96    0.052380    0.052354    0.052355    0.052366    0.790546\n",
       "97    0.073314    0.073301    0.073293    0.706780    0.073313\n",
       "98    0.817572    0.045743    0.045540    0.045543    0.045603\n",
       "99    0.813676    0.046672    0.046550    0.046572    0.046530\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算 n_top_words 个主题词\n",
    "top_words_df = top_words_data_frame(lda, tf_idf_vectorizer, n_top_words)\n",
    "top_words_df\n",
    "# 保存 n_top_words 个主题词到 csv 文件中\n",
    "#top_words_df.to_csv(top_words_csv_path, encoding='utf-8-sig', index=None)\n",
    "\n",
    "# 转 tf_idf 为数组，以便后面使用它来对文本主题概率分布进行计算\n",
    "X = tf_idf.toarray()\n",
    "\n",
    "# 计算完毕主题概率分布情况\n",
    "predict_df = predict_to_data_frame(lda, X)\n",
    "predict_df\n",
    "# 保存文本主题概率分布到 csv 文件中\n",
    "#predict_df.to_csv(predict_topic_csv_path, encoding='utf-8-sig', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f268fbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "      <th>topic 10</th>\n",
       "      <th>topic 11</th>\n",
       "      <th>topic 12</th>\n",
       "      <th>topic 13</th>\n",
       "      <th>topic 14</th>\n",
       "      <th>topic 15</th>\n",
       "      <th>topic 16</th>\n",
       "      <th>topic 17</th>\n",
       "      <th>topic 18</th>\n",
       "      <th>topic 19</th>\n",
       "      <th>topic 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>物资</td>\n",
       "      <td>健健康康</td>\n",
       "      <td>快递</td>\n",
       "      <td>真的</td>\n",
       "      <td>辞职</td>\n",
       "      <td>第一天</td>\n",
       "      <td>希望</td>\n",
       "      <td>天气</td>\n",
       "      <td>疫情</td>\n",
       "      <td>东西</td>\n",
       "      <td>不要脸</td>\n",
       "      <td>这是</td>\n",
       "      <td>晃晃</td>\n",
       "      <td>胡说八道</td>\n",
       "      <td>出门</td>\n",
       "      <td>开箱</td>\n",
       "      <td>京东</td>\n",
       "      <td>日上</td>\n",
       "      <td>张老师</td>\n",
       "      <td>决赛圈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>办公</td>\n",
       "      <td>妈妈</td>\n",
       "      <td>疫情</td>\n",
       "      <td>民生</td>\n",
       "      <td>居家</td>\n",
       "      <td>健康</td>\n",
       "      <td>上班</td>\n",
       "      <td>平安</td>\n",
       "      <td>结束</td>\n",
       "      <td>老板</td>\n",
       "      <td>黑市</td>\n",
       "      <td>跑腿</td>\n",
       "      <td>星期天</td>\n",
       "      <td>孩子</td>\n",
       "      <td>肉眼</td>\n",
       "      <td>一圈</td>\n",
       "      <td>物资</td>\n",
       "      <td>代购</td>\n",
       "      <td>明康汇</td>\n",
       "      <td>开心</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>端午</td>\n",
       "      <td>妞妞</td>\n",
       "      <td>花草</td>\n",
       "      <td>家有</td>\n",
       "      <td>水彩</td>\n",
       "      <td>押花</td>\n",
       "      <td>疫情</td>\n",
       "      <td>仓鼠</td>\n",
       "      <td>邻居</td>\n",
       "      <td>返乡</td>\n",
       "      <td>隔离</td>\n",
       "      <td>脑残</td>\n",
       "      <td>真特</td>\n",
       "      <td>物价</td>\n",
       "      <td>连花清</td>\n",
       "      <td>离谱</td>\n",
       "      <td>一位</td>\n",
       "      <td>感谢</td>\n",
       "      <td>喜提</td>\n",
       "      <td>礼物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>疫情</td>\n",
       "      <td>民生</td>\n",
       "      <td>团长</td>\n",
       "      <td>封控</td>\n",
       "      <td>新闻</td>\n",
       "      <td>小区</td>\n",
       "      <td>细节</td>\n",
       "      <td>今晚</td>\n",
       "      <td>徐汇</td>\n",
       "      <td>锅包肉</td>\n",
       "      <td>人世间</td>\n",
       "      <td>载入史册</td>\n",
       "      <td>真他妈</td>\n",
       "      <td>垃圾</td>\n",
       "      <td>烦透了</td>\n",
       "      <td>网友</td>\n",
       "      <td>成本</td>\n",
       "      <td>演员</td>\n",
       "      <td>过分</td>\n",
       "      <td>去年</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>快乐</td>\n",
       "      <td>适合</td>\n",
       "      <td>果园</td>\n",
       "      <td>疫情</td>\n",
       "      <td>不行</td>\n",
       "      <td>周四</td>\n",
       "      <td>周一</td>\n",
       "      <td>家门</td>\n",
       "      <td>美景</td>\n",
       "      <td>无语</td>\n",
       "      <td>守护星</td>\n",
       "      <td>天使</td>\n",
       "      <td>措施</td>\n",
       "      <td>隔离</td>\n",
       "      <td>核酸</td>\n",
       "      <td>一顿</td>\n",
       "      <td>三顿</td>\n",
       "      <td>居家</td>\n",
       "      <td>防疫</td>\n",
       "      <td>善良</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic 1 topic 2 topic 3 topic 4 topic 5 topic 6 topic 7 topic 8 topic 9  \\\n",
       "0      物资    健健康康      快递      真的      辞职     第一天      希望      天气      疫情   \n",
       "1      办公      妈妈      疫情      民生      居家      健康      上班      平安      结束   \n",
       "2      端午      妞妞      花草      家有      水彩      押花      疫情      仓鼠      邻居   \n",
       "3      疫情      民生      团长      封控      新闻      小区      细节      今晚      徐汇   \n",
       "4      快乐      适合      果园      疫情      不行      周四      周一      家门      美景   \n",
       "\n",
       "  topic 10 topic 11 topic 12 topic 13 topic 14 topic 15 topic 16 topic 17  \\\n",
       "0       东西      不要脸       这是       晃晃     胡说八道       出门       开箱       京东   \n",
       "1       老板       黑市       跑腿      星期天       孩子       肉眼       一圈       物资   \n",
       "2       返乡       隔离       脑残       真特       物价      连花清       离谱       一位   \n",
       "3      锅包肉      人世间     载入史册      真他妈       垃圾      烦透了       网友       成本   \n",
       "4       无语      守护星       天使       措施       隔离       核酸       一顿       三顿   \n",
       "\n",
       "  topic 18 topic 19 topic 20  \n",
       "0       日上      张老师      决赛圈  \n",
       "1       代购      明康汇       开心  \n",
       "2       感谢       喜提       礼物  \n",
       "3       演员       过分       去年  \n",
       "4       居家       防疫       善良  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7df8f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d58142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\anaconda\\lib\\site-packages\\pyLDAvis\\_prepare.py:228: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info  = pd.DataFrame({'saliency': saliency, 'Term': vocab, \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可视化 html 文件路径\n",
    "html_path = 'document-lda-visualization.html'\n",
    "# 使用 pyLDAvis 进行可视化\n",
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "pyLDAvis.save_html(data, html_path)\n",
    "# 清屏\n",
    "os.system('clear')\n",
    "# 浏览器打开 html 文件以查看可视化结果\n",
    "os.system(f'start {html_path}')\n",
    "\n",
    "#print('本次生成了文件：',\n",
    "#      top_words_csv_path,\n",
    "#      predict_topic_csv_path,\n",
    "#      html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "058c1128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el113922430309943104583974500\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el113922430309943104583974500_data = {\"mdsDat\": {\"x\": [0.04708297478012252, -0.01920103808700884, -0.011507286918303565, -0.007863814479727907, -0.008510835295082215], \"y\": [-0.0036172105104201615, -0.023121052302802557, 0.017007100539405188, 0.007552663129616729, 0.0021784991442008158], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.487853162108, 24.128982730558, 17.81624693247367, 16.3411199393061, 15.225797235554223]}, \"tinfo\": {\"Term\": [\"\\u599e\\u599e\", \"\\u7aef\\u5348\", \"\\u82b1\\u8349\", \"\\u5bb6\\u6709\", \"\\u62bc\\u82b1\", \"\\u6c34\\u5f69\", \"\\u7269\\u8d44\", \"\\u529e\\u516c\", \"\\u4ed3\\u9f20\", \"\\u5feb\\u4e50\", \"\\u9002\\u5408\", \"\\u5988\\u5988\", \"\\u679c\\u56ed\", \"\\u5feb\\u9012\", \"\\u56e2\\u957f\", \"\\u4e0d\\u884c\", \"\\u5c01\\u63a7\", \"\\u8f9e\\u804c\", \"\\u5468\\u56db\", \"\\u5065\\u5065\\u5eb7\\u5eb7\", \"\\u7b2c\\u4e00\\u5929\", \"\\u771f\\u7684\", \"\\u5468\\u4e00\", \"\\u7f8e\\u666f\", \"\\u5bb6\\u95e8\", \"\\u5929\\u6c14\", \"\\u5973\\u795e\", \"\\u5e0c\\u671b\", \"\\u65e0\\u8bed\", \"\\u4e1c\\u897f\", \"\\u599e\\u599e\", \"\\u82b1\\u8349\", \"\\u7aef\\u5348\", \"\\u5bb6\\u6709\", \"\\u62bc\\u82b1\", \"\\u6c34\\u5f69\", \"\\u4ed3\\u9f20\", \"\\u8fd4\\u4e61\", \"\\u8111\\u6b8b\", \"\\u771f\\u7279\", \"\\u7269\\u4ef7\", \"\\u79bb\\u8c31\", \"\\u8fde\\u82b1\\u6e05\", \"\\u4e00\\u4f4d\", \"\\u559c\\u63d0\", \"\\u793c\\u7269\", \"\\u62ff\\u634f\", \"\\u516c\\u53f8\", \"\\u90bb\\u5c45\", \"\\u521d\\u957f\\u6210\", \"\\u7816\\u5bb6\", \"\\u591c\\u5954\", \"\\u4f17\\u4eba\", \"\\u51b0\\u6dc7\\u6dcb\", \"\\u5973\\u5a03\\u5a03\", \"\\u5c0f\\u77e5\", \"\\u559d\\u5976\", \"\\u7ad9\\u53f0\", \"\\u62fe\\u67f4\", \"\\u5c6f\\u83dc\", \"\\u75ab\\u60c5\", \"\\u9694\\u79bb\", \"\\u611f\\u8c22\", \"\\u540c\\u4e8b\", \"\\u529e\\u516c\", \"\\u5988\\u5988\", \"\\u5065\\u5eb7\", \"\\u5e73\\u5b89\", \"\\u8dd1\\u817f\", \"\\u9ed1\\u5e02\", \"\\u661f\\u671f\\u5929\", \"\\u8089\\u773c\", \"\\u4e00\\u5708\", \"\\u660e\\u5eb7\\u6c47\", \"\\u9001\\u8d27\", \"\\u665a\\u4e0a\", \"\\u5c45\\u5bb6\", \"\\u5413\\u4eba\", \"\\u5e73\\u590d\", \"\\u4e0a\\u73ed\", \"\\u653f\\u5e9c\", \"\\u5e05\\u4e0d\\u5e05\", \"\\u5ba3\\u514b\\u7085\", \"\\u7238\\u7238\", \"\\u5931\\u7720\", \"\\u7ed3\\u675f\", \"\\u6625\\u5929\", \"\\u767b\\u9646\", \"\\u4e45\\u4e45\", \"\\u611f\\u6069\", \"\\u5fae\\u4fe1\", \"\\u5e72\\u6270\", \"\\u8001\\u677f\", \"\\u6559\\u6388\", \"\\u4ee3\\u8d2d\", \"\\u5f00\\u5fc3\", \"\\u6c11\\u751f\", \"\\u75ab\\u60c5\", \"\\u5b69\\u5b50\", \"\\u7269\\u8d44\", \"\\u65e9\\u65e5\", \"\\u540c\\u4e8b\", \"\\u56e2\\u957f\", \"\\u5c01\\u63a7\", \"\\u65b0\\u95fb\", \"\\u5f90\\u6c47\", \"\\u7ec6\\u8282\", \"\\u4eba\\u4e16\\u95f4\", \"\\u9505\\u5305\\u8089\", \"\\u4eca\\u665a\", \"\\u8f7d\\u5165\\u53f2\\u518c\", \"\\u5783\\u573e\", \"\\u771f\\u4ed6\\u5988\", \"\\u70e6\\u900f\\u4e86\", \"\\u6210\\u672c\", \"\\u6f14\\u5458\", \"\\u7f51\\u53cb\", \"\\u8fc7\\u5206\", \"\\u53bb\\u5e74\", \"\\u9a8c\\u8bc1\", \"\\u7537\\u751f\", \"\\u5929\\u5929\", \"\\u53d1\\u751f\", \"\\u8c23\\u8a00\", \"\\u5ba0\\u7231\", \"\\u82f9\\u679c\", \"\\u9633\\u5149\", \"\\u5973\\u751f\", \"\\u8c22\\u8c22\", \"\\u8f9f\\u8c23\", \"\\u6211\\u53f8\", \"\\u6551\\u4eba\", \"\\u5c0f\\u533a\", \"\\u6c11\\u751f\", \"\\u75ab\\u60c5\", \"\\u9632\\u63a7\", \"\\u9002\\u5408\", \"\\u679c\\u56ed\", \"\\u4e0d\\u884c\", \"\\u5468\\u56db\", \"\\u7f8e\\u666f\", \"\\u5bb6\\u95e8\", \"\\u5468\\u4e00\", \"\\u5feb\\u4e50\", \"\\u65e0\\u8bed\", \"\\u5929\\u4f7f\", \"\\u5b88\\u62a4\\u661f\", \"\\u4e00\\u987f\", \"\\u53ef\\u7231\", \"\\u5584\\u826f\", \"\\u8fc7\\u654f\", \"\\u63aa\\u65bd\", \"\\u4e00\\u5957\", \"\\u8d81\\u706b\\u6253\\u52ab\", \"\\u4eea\\u5f0f\", \"\\u60ca\\u559c\", \"\\u9a91\\u624b\", \"\\u548b\\u5566\", \"\\u9633\\u6027\\u53cd\\u5e94\", \"\\u80fd\\u91cf\", \"\\u8d70\\u8def\", \"\\u8b66\\u65b9\", \"\\u978b\\u5b50\", \"\\u54d0\\u54cd\", \"\\u8fd9\\u624d\", \"\\u4f9d\\u65e7\", \"\\u9632\\u75ab\", \"\\u6838\\u9178\", \"\\u4e09\\u987f\", \"\\u9694\\u79bb\", \"\\u75ab\\u60c5\", \"\\u5c45\\u5bb6\", \"\\u90bb\\u5c45\", \"\\u5973\\u795e\", \"\\u5feb\\u9012\", \"\\u8f9e\\u804c\", \"\\u7b2c\\u4e00\\u5929\", \"\\u7269\\u8d44\", \"\\u4e1c\\u897f\", \"\\u80e1\\u8bf4\\u516b\\u9053\", \"\\u4e0d\\u8981\\u8138\", \"\\u51fa\\u95e8\", \"\\u6643\\u6643\", \"\\u4eac\\u4e1c\", \"\\u5f00\\u7bb1\", \"\\u5f20\\u8001\\u5e08\", \"\\u5065\\u5065\\u5eb7\\u5eb7\", \"\\u65e5\\u4e0a\", \"\\u5e73\\u5e73\\u5b89\\u5b89\", \"\\u67aa\\u6bd9\", \"\\u8282\\u540e\", \"\\u5e0c\\u671b\", \"\\u665a\\u5b89\", \"\\u6709\\u65f6\\u5019\", \"\\u5929\\u6c14\", \"\\u8fd9\\u662f\", \"\\u771f\\u7684\", \"\\u51b3\\u8d5b\\u5708\", \"\\u4e24\\u5e74\", \"\\u7f51\\u4f20\", \"\\u53ed\\u53ed\", \"\\u4e13\\u4e1a\", \"\\u5b9e\\u529b\", \"\\u5c11\\u5973\", \"\\u52a0\\u6cb9\", \"\\u5973\\u795e\", \"\\u65e9\\u65e5\", \"\\u9632\\u63a7\", \"\\u6297\\u75ab\", \"\\u75ab\\u60c5\", \"\\u5feb\\u4e50\"], \"Freq\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3893408648393641, 1.3873666863519438, 1.3921943294646202, 1.335925614381461, 1.1674065753778253, 1.1685418000495373, 0.551165160546518, 0.2786626725736369, 0.27099497391394994, 0.2666956291313379, 0.2648725183584953, 0.2554150859860588, 0.25712010495179066, 0.2497640432904383, 0.2426812958369546, 0.2367227171792114, 0.2338419898077367, 0.23192907770181048, 0.3024618782120575, 0.22790193006999282, 0.22844529219142318, 0.22079162648764394, 0.2094531657576452, 0.2108554788909858, 0.21624339768595693, 0.2166294025079567, 0.21418985129841878, 0.2080712147341486, 0.2081430060047234, 0.212862135720091, 0.7584232607890298, 0.27443707876284534, 0.24471048394541864, 0.2200211719336792, 0.4872131061264028, 0.4362866765258096, 0.3016299006798899, 0.2573965367324333, 0.24249390172851326, 0.24321039202176442, 0.23980829421848174, 0.2244013563343626, 0.22288533840794741, 0.2146339836190683, 0.21445891197131542, 0.20392237472632363, 0.32725037181389605, 0.20115326999438363, 0.2001517749367553, 0.30083374746736485, 0.19933827169255264, 0.20424622616047602, 0.19497668239099875, 0.1978305999911048, 0.19730633313966905, 0.24565929254892116, 0.2033757294456732, 0.19863155711745922, 0.19996019133678558, 0.18789078956719868, 0.1977655900100699, 0.18166494498524655, 0.24459600876614554, 0.17673989829448464, 0.21988691386725084, 0.2146313091554146, 0.36548147193531505, 0.43162301078389315, 0.22803406973183482, 0.2217853663797833, 0.2019848192807804, 0.20016597715668996, 0.24511236931569946, 0.23939071713868998, 0.19603083622344084, 0.1784306223921622, 0.1795695312339935, 0.17648326673930229, 0.17658116155054904, 0.1791096386544966, 0.17529960025689997, 0.16905171799415133, 0.16919445326492702, 0.16645455182117372, 0.16289753750588126, 0.1605284051693202, 0.16479676207505517, 0.1566902557287615, 0.1561508524674428, 0.15606687694533325, 0.15140493355737045, 0.15147436217527363, 0.15333596543159447, 0.15370199039942084, 0.15137048570700318, 0.15042278529579126, 0.14871058480068863, 0.15121907683129307, 0.14851488128847315, 0.15108582131074869, 0.15089255040852956, 0.14212526253291535, 0.18580781941516158, 0.24801758644722063, 0.2550404485222016, 0.15575321844059734, 0.25024424658477384, 0.2270872928886509, 0.20929579359843944, 0.20508731858855062, 0.19260424892376085, 0.19291509566784495, 0.19727934976150593, 0.2622753607872014, 0.1773012033494771, 0.16777003484832237, 0.1686620233828166, 0.15977891586915086, 0.1553066253922261, 0.15575845720546025, 0.15249130034311453, 0.16475263028436732, 0.141007465829267, 0.13860408375993674, 0.1404573179771722, 0.13617423381723312, 0.131220998522156, 0.1348863378143796, 0.13324818379806716, 0.15328211993949659, 0.1253356805622034, 0.12779078097680044, 0.1245785115088257, 0.12583040250433022, 0.1253071787615807, 0.12391403678377878, 0.15696311944539246, 0.16164711761456335, 0.15922642772190837, 0.16316256112477925, 0.22590517279907857, 0.1574697263089456, 0.1410808812324242, 0.1341492773525468, 0.1989550437093078, 0.18251553110845745, 0.18068494584210618, 0.27137473771453996, 0.15586983622617795, 0.15362182693749218, 0.15576573157469997, 0.15351129634412267, 0.1538238941825232, 0.14953954662927, 0.15211615300222608, 0.1485924454529407, 0.20997354273511468, 0.1491179903170587, 0.1429874061170569, 0.14128216753224543, 0.1416118577966261, 0.17786751304403373, 0.13426520504436498, 0.1338382083091915, 0.17185210263320458, 0.1543090262491421, 0.19004374121325734, 0.14591529158271171, 0.1157594301120543, 0.1138700901613165, 0.11182607968918161, 0.13535830192771703, 0.11178928911899813, 0.11048571673696793, 0.13448528414672115, 0.1406222192097422, 0.13978308081481478, 0.12984214142821665, 0.13170545704500497, 0.15872841891822548, 0.13246181523010522], \"Total\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6291689306328097, 1.6292679560416745, 1.6363344096720076, 1.581833965441168, 1.411814632693876, 1.4140409792180202, 0.787926777551926, 0.5102609815777124, 0.5061990191894837, 0.5055408266090439, 0.5033768789240585, 0.48898398227868434, 0.4987597904419781, 0.48938776028666375, 0.4795046209507524, 0.47027182971472464, 0.4777276018881667, 0.4755436746559676, 0.6247154138302493, 0.4739856381210326, 0.4751420000350591, 0.4607878688233327, 0.4417189569021044, 0.4449216758351872, 0.4575617577143653, 0.4596857205764682, 0.45509972528312287, 0.4433046252673929, 0.44596376643058405, 0.4627340990151618, 1.8297203118124286, 0.6207499804616944, 0.5963232124849609, 0.5918376335411054, 0.7361585320740869, 0.6823940572802907, 0.5751400637227388, 0.5025598570227312, 0.4806413089242287, 0.4880133391135491, 0.48746854168909404, 0.46277736362354427, 0.4695744974490212, 0.4565412073718799, 0.4561739272430398, 0.4403073550004835, 0.7081896329465718, 0.4410476583015007, 0.4402476560978719, 0.6629517303538814, 0.44008373017934044, 0.45185727970948525, 0.43141702833464785, 0.43834191779442033, 0.4381962012227424, 0.5459154719844508, 0.4521895968425914, 0.44190069427485135, 0.4491890620630983, 0.4263031165388067, 0.4528378305716706, 0.4212543536654734, 0.5697854439688226, 0.41474885398918027, 0.51831713463375, 0.512119307964559, 0.9633394806288649, 1.8297203118124286, 0.6411710977835406, 0.681753598931607, 0.5281712387165118, 0.5918376335411054, 0.4978304127049925, 0.5027193786982884, 0.45992463015389534, 0.43256330922674463, 0.4354424177059638, 0.43033013259631714, 0.43296951224964675, 0.443462256461338, 0.4357562948777648, 0.42049713849538095, 0.42659006792287935, 0.41973738925522264, 0.4202776446576863, 0.41449245516495864, 0.42599385834128417, 0.4069293355729731, 0.4084647293263117, 0.41182822744549236, 0.4022574561784835, 0.4040878969551761, 0.40946096884557026, 0.41093976873214255, 0.4059514916764038, 0.40449384737346566, 0.40218494984296715, 0.4106148620214858, 0.40394808214222755, 0.41094446137552487, 0.41653216547800326, 0.3935280737962324, 0.5786520087937123, 0.9633394806288649, 1.8297203118124286, 0.4906086466414522, 0.5154893492928974, 0.4842397397840049, 0.4638658792292484, 0.46535514119303895, 0.44740811128218444, 0.44955469061660475, 0.4608104748529122, 0.6454998498924936, 0.4375787217856477, 0.42481093786399327, 0.4309926767049257, 0.4192742347030664, 0.4135420235847613, 0.4177259822199603, 0.4134520011861856, 0.45968339413852305, 0.40367043345912407, 0.39869064556143435, 0.4062631468870005, 0.3956263968597198, 0.38557785083021245, 0.39771577559341253, 0.3929287101538277, 0.4619422965360457, 0.3795803065938021, 0.38800381713525167, 0.37853849945245704, 0.3827656947563111, 0.3835793968758248, 0.38125739683703963, 0.48900637410958037, 0.5467695412081938, 0.5400000959211869, 0.6207499804616944, 1.8297203118124286, 0.7081896329465718, 0.6247154138302493, 0.48608932404344807, 0.46135081290490676, 0.4401210965221819, 0.44648557822223245, 0.681753598931607, 0.4198384442732247, 0.4139392972135941, 0.41996260837390764, 0.41395613108668095, 0.41528801488922584, 0.4050961368460334, 0.4184766779283822, 0.41015698353492414, 0.5799213035952884, 0.4122713032113716, 0.40422796249360676, 0.40454306348810004, 0.40582008061779445, 0.5128854146730071, 0.3973946177044184, 0.4006679747441425, 0.5195307253619386, 0.4785306924359605, 0.5974383826677827, 0.47133765452848225, 0.37691830380783875, 0.37548024539604075, 0.37569913831165896, 0.4548407386018787, 0.3769036544854787, 0.3763167041010216, 0.4622764827631418, 0.48608932404344807, 0.5281712387165118, 0.4906086466414522, 0.5257952792850658, 1.8297203118124286, 0.6454998498924936], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9953, -3.9967, -3.9933, -4.0345, -4.1694, -4.1684, -4.9199, -5.6019, -5.6298, -5.6458, -5.6527, -5.689, -5.6824, -5.7114, -5.7402, -5.765, -5.7773, -5.7855, -5.52, -5.803, -5.8006, -5.8347, -5.8874, -5.8807, -5.8555, -5.8537, -5.865, -5.894, -5.8937, -5.8713, -4.6007, -5.6172, -5.7318, -5.8382, -4.9499, -5.0603, -5.4294, -5.588, -5.6477, -5.6447, -5.6588, -5.7252, -5.732, -5.7697, -5.7705, -5.8209, -5.3479, -5.8346, -5.8396, -5.4321, -5.8436, -5.8193, -5.8658, -5.8512, -5.8539, -5.6347, -5.8236, -5.8472, -5.8405, -5.9028, -5.8516, -5.9365, -5.639, -5.964, -5.7455, -5.7697, -5.2374, -5.0711, -5.7091, -5.7369, -5.8304, -5.8395, -5.3336, -5.3572, -5.5571, -5.6511, -5.6448, -5.6621, -5.6616, -5.6473, -5.6688, -5.7051, -5.7043, -5.7206, -5.7422, -5.7569, -5.7306, -5.7811, -5.7845, -5.785, -5.8154, -5.8149, -5.8027, -5.8003, -5.8156, -5.8219, -5.8333, -5.8166, -5.8346, -5.8175, -5.8188, -5.8786, -5.6106, -5.3218, -5.2939, -5.7871, -5.2265, -5.3236, -5.4052, -5.4255, -5.4883, -5.4867, -5.4643, -5.1795, -5.5711, -5.6263, -5.621, -5.6751, -5.7035, -5.7006, -5.7218, -5.6445, -5.8001, -5.8173, -5.804, -5.835, -5.872, -5.8445, -5.8567, -5.7166, -5.9179, -5.8985, -5.924, -5.914, -5.9181, -5.9293, -5.6929, -5.6635, -5.6786, -5.6542, -5.3288, -5.6897, -5.7996, -5.85, -5.3851, -5.4714, -5.4815, -5.0747, -5.6292, -5.6437, -5.6299, -5.6444, -5.6424, -5.6707, -5.6536, -5.677, -5.3312, -5.6735, -5.7155, -5.7275, -5.7251, -5.4972, -5.7784, -5.7816, -5.5316, -5.6393, -5.431, -5.6952, -5.9267, -5.9432, -5.9613, -5.7703, -5.9616, -5.9733, -5.7768, -5.7321, -5.7381, -5.8119, -5.7976, -5.611, -5.7919], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1692, 1.1678, 1.1669, 1.1595, 1.1384, 1.1378, 0.9711, 0.7236, 0.7037, 0.689, 0.6864, 0.679, 0.6659, 0.6558, 0.6475, 0.6421, 0.6141, 0.6105, 0.6031, 0.5962, 0.5962, 0.5928, 0.5823, 0.5818, 0.579, 0.5761, 0.5748, 0.5721, 0.5665, 0.552, 0.4478, 0.5123, 0.4378, 0.339, 1.009, 0.9744, 0.7763, 0.7527, 0.7376, 0.7253, 0.7124, 0.6979, 0.6766, 0.667, 0.667, 0.652, 0.6498, 0.6367, 0.6335, 0.6316, 0.6298, 0.6277, 0.6276, 0.6262, 0.6238, 0.6232, 0.6227, 0.6221, 0.6124, 0.6025, 0.5933, 0.5807, 0.5761, 0.5688, 0.5643, 0.5521, 0.4526, -0.0226, 0.388, 0.2988, 0.4605, 0.3377, 1.0165, 0.9831, 0.8723, 0.8395, 0.8393, 0.8337, 0.8282, 0.8184, 0.8145, 0.8138, 0.8003, 0.8002, 0.7773, 0.7765, 0.7753, 0.7707, 0.7635, 0.7547, 0.7479, 0.7438, 0.7428, 0.7416, 0.7386, 0.7359, 0.7301, 0.7261, 0.7245, 0.7244, 0.7097, 0.7066, 0.5891, 0.3682, -0.2454, 0.5777, 1.0888, 1.0542, 1.0156, 0.9921, 0.9687, 0.9655, 0.9631, 0.9109, 0.9081, 0.8824, 0.8733, 0.8468, 0.8321, 0.825, 0.8141, 0.7854, 0.7597, 0.7549, 0.7494, 0.745, 0.7336, 0.7302, 0.7301, 0.7083, 0.7034, 0.7009, 0.7001, 0.699, 0.6927, 0.6876, 0.6751, 0.5929, 0.5902, 0.4753, -0.2803, 0.308, 0.3235, 0.524, 1.0411, 1.002, 0.9775, 0.961, 0.8913, 0.891, 0.8904, 0.8902, 0.889, 0.8856, 0.8702, 0.8668, 0.8663, 0.8652, 0.843, 0.8302, 0.8294, 0.8232, 0.7971, 0.7857, 0.7759, 0.7504, 0.7368, 0.7096, 0.7017, 0.689, 0.6703, 0.6702, 0.6668, 0.6566, 0.6475, 0.6419, 0.5529, 0.5529, 0.4978, -0.5625, 0.2984]}, \"token.table\": {\"Topic\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [1.2691534651315972, 0.6138098887090703, 0.6321776000815, 0.7083082841349401, 0.7071930832959387, 0.5465316166324079, 0.6111220262125047, 0.6137725819082035], \"Term\": [\"\\u4ed3\\u9f20\", \"\\u599e\\u599e\", \"\\u5bb6\\u6709\", \"\\u62bc\\u82b1\", \"\\u6c34\\u5f69\", \"\\u75ab\\u60c5\", \"\\u7aef\\u5348\", \"\\u82b1\\u8349\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 5, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el113922430309943104583974500\", ldavis_el113922430309943104583974500_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el113922430309943104583974500\", ldavis_el113922430309943104583974500_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el113922430309943104583974500\", ldavis_el113922430309943104583974500_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "#让可视化可以在notebook内显示\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51623)\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\http\\server.py\", line 427, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\http\\server.py\", line 395, in handle_one_request\n",
      "    self.raw_requestline = self.rfile.readline(65537)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51622)\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\http\\server.py\", line 427, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\http\\server.py\", line 395, in handle_one_request\n",
      "    self.raw_requestline = self.rfile.readline(65537)\n",
      "  File \"D:\\Program_Files\\anaconda\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。\n",
      "----------------------------------------\n",
      "127.0.0.1 - - [11/Oct/2022 21:36:12] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2022 21:38:32] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2022 21:38:32] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2022 21:38:32] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Oct/2022 21:38:32] code 404, message Not Found\n",
      "127.0.0.1 - - [11/Oct/2022 21:38:32] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67f874",
   "metadata": {},
   "source": [
    "## 生活满意度预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3f3d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493824d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tt['user_id']\n",
    "b= list(set(a))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写循环，把id 一致的行写入一个txt中\n",
    "#模型需要输入的特征是词频比例，需要每个人的所有帖子合在一起\n",
    "def pick_id(dataframe,id):\n",
    "    pick = []\n",
    "    all = []\n",
    "    id_all = pd.DataFrame(columns=(['id','content']))\n",
    "    length = len(id)\n",
    "    for i in range(length):\n",
    "        u_id = id[i]\n",
    "        for idx in range(len(dataframe['user_id'])):\n",
    "            num = dataframe['user_id'][idx]\n",
    "            if num == u_id:\n",
    "                pick.append(dataframe['del'][idx])\n",
    "                \n",
    "        sum = \"\"\n",
    "        for aa in pick:\n",
    "            sum += (aa + \" \")\n",
    "        all.append(sum)\n",
    "        pick = []\n",
    "        i += 1\n",
    "    id_all['content'] = pd.DataFrame(all)\n",
    "    id_all['id'] = pd.DataFrame(id)\n",
    "    return id_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a253dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pick_id(tt,b)\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60225bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征提取\n",
    "def feature_extraction_new(words):\n",
    "    item = []\n",
    "    #切词后词的总数\n",
    "    cnt_tags = len(words)\n",
    "    #print(cnt_tags)\n",
    "     \n",
    "    #提取每个情感词类的词频比率\n",
    "    idx = 0\n",
    "    for g_col in affect_col_list:\n",
    "        \n",
    "        #切词后，其中包含情感词的总数\n",
    "        affect_cnt = 0\n",
    "        \n",
    "        #统计每个词类下关键词出现的总频次affect_cnt\n",
    "        for i in range(cnt_tags):\n",
    "            s = words[i]\n",
    "            if(s in affect_dict[idx]):\n",
    "                affect_cnt += 1\n",
    "        \n",
    "        #计算比率\n",
    "        r_affect = 0.0\n",
    "        if (cnt_tags > 0):\n",
    "            r_affect = affect_cnt/cnt_tags\n",
    "         \n",
    "        item.append(r_affect)\n",
    "        idx += 1\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  pi['content'][1]\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18270ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_file = 'D:/ww/BDP-Resource/data/swls.mod'\n",
    "clf = joblib.load(mod_file)\n",
    "score = []\n",
    "#word = []\n",
    "for i in range(len(pi['content'])):\n",
    "    uu = pi['content'][i]\n",
    "    word = uu.split(' ')\n",
    "    item_feature = feature_extraction_new(word)\n",
    "    reshape = np.array(item_feature).reshape(1,-1)\n",
    "    result = clf.predict(reshape)\n",
    "    score.append(result)\n",
    "    \n",
    "pi['score'] = pd.DataFrame(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ee305",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d75ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
